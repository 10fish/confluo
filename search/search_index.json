{
    "docs": [
        {
            "location": "/", 
            "text": "Confluo Overview\n\n\nConfluo is a system for real-time monitoring and analysis of data, that supports:\n\n\n\n\nhigh-throughput concurrent writes of millions of data points from multiple data streams;\n\n\nonline queries at millisecond timescale; and \n\n\nad-hoc queries using minimal CPU resources.\n\n\n\n\nUser Guide\n\n\n\n\nQuick Start\n\n\nInstallation\n\n\nModes of Operation\n\n\nData Storage\n\n\nType System\n\n\n\n\n\n\nQuerying Data\n\n\nOnline Queries\n\n\nOffline Queries\n\n\n\n\n\n\nArchiving Data\n\n\n\n\nAPI Docs\n\n\n\n\nC++ API\n\n\nClient APIs\n\n\nC++ Client API\n\n\nPython Client API\n\n\nJava Client API", 
            "title": "Overview"
        }, 
        {
            "location": "/#confluo-overview", 
            "text": "Confluo is a system for real-time monitoring and analysis of data, that supports:   high-throughput concurrent writes of millions of data points from multiple data streams;  online queries at millisecond timescale; and   ad-hoc queries using minimal CPU resources.", 
            "title": "Confluo Overview"
        }, 
        {
            "location": "/#user-guide", 
            "text": "Quick Start  Installation  Modes of Operation  Data Storage  Type System    Querying Data  Online Queries  Offline Queries    Archiving Data", 
            "title": "User Guide"
        }, 
        {
            "location": "/#api-docs", 
            "text": "C++ API  Client APIs  C++ Client API  Python Client API  Java Client API", 
            "title": "API Docs"
        }, 
        {
            "location": "/quick_start/", 
            "text": "Quick Start\n\n\nIn this Quick Start, we will take a look at how to download and setup Confluo,\nload some sample data, and query it.\n\n\nPre-requisites\n\n\n\n\nMacOS X or Unix-based OS; Windows is not yet supported.\n\n\nC++ compiler that supports C++11 standard\n\n\nCMake 2.8 or later\n\n\nBoost 1.53 or later\n\n\n\n\nFor python client, you will additionally require:\n\n\n\n\nPython 2.7 or later\n\n\nPython Packages: six 1.7.2 or later\n\n\n\n\nFor java client, you will additionally require:\n\n\n\n\nJava 1.7 or later\n\n\nant 1.6.2 or later\n\n\n\n\nDownload and Install\n\n\nTo download and install Confluo, use the following commands:\n\n\ngit clone https://github.com/ucbrise/confluo.git\n\ncd\n confluo\nmkdir build\n\ncd\n build\ncmake ..\nmake -j \n make \ntest\n \n make install\n\n\n\n\n\nUsing Confluo\n\n\nConfluo can be used in two modes -- embedded and stand-alone. In the embedded mode,\nConfluo is used as a header-only library in C++, allowing Confluo to use the same\naddress-space as the application process. In the stand-alone mode, Confluo runs\nas a daemon server process, allowing clients to communicate with it using \n\nApache Thrift\n protocol.\n\n\nEmbedded Mode\n\n\nIn order to use Confluo in the embedded mode, we simply need to include\nConfluo's header files under libconfluo/confluo, use the Confluo C++ API in \na C++ application, and compile using a modern C++ compiler. The entry point \nheader file to include is \nconfluo_store.h\n. \n\n\nWe will first create a new Confluo Store with the data path for it to use \nas follows:\n\n\nconfluo\n::\nconfluo_store\n \nstore\n(\n/path/to/data\n);\n\n\n\n\n\n\nWe then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); this requires three parameters: name, schema, and the storage mode:\n\n\nstd\n::\nstring\n \nschema\n(\n{ type: CHAR, msg: STRING(100) }\n);\n\n\nauto\n \nstorage_mode\n \n=\n \nconfluo\n::\nstorage\n::\nIN_MEMORY\n;\n\n\nstore\n.\ncreate_atomic_multilog\n(\nmy_multilog\n,\n \nschema\n,\n \nstorage_mode\n);\n\n\n\n\n\n\nOur simple schema only contains two attributes: a character type attribute, and \na string message field. Confluo will also automatically create a timestamp \nfield if the schema does not explicityly specify one. \n\n\nWe then obtain a reference to our newly created Atomic MultiLog:\n\n\nconfluo\n::\natomic_multilog\n*\n \nmlog\n \n=\n \nstore\n.\nget_atomic_multilog\n(\nmy_multilog\n);\n\n\n\n\n\n\nWe can define indexes on the Atomic MultiLog as follows:\n\n\nmlog\n-\nadd_index\n(\ntype\n)\n\n\n\n\n\n\nto add an index on \"type\" attribute. We can also install filters as follows:\n\n\nmlog\n-\nadd_filter\n(\ntype_a\n,\n \ntype == a\n);\n\n\n\n\n\n\nto explicitly filter out records that have the \"type\" attribute value \"a\", using\na filter named \"type_a\". \n\n\nAdditionally, we can add aggregates on filters as follows:\n\n\nmlog\n-\nadd_aggregate\n(\nlatest_a\n,\n \ntype_a\n,\n \nMAX(timestamp)\n)\n\n\n\n\n\n\nThis adds a new stored aggregate \"latest_a\" on the filter \"type_a\" we defined \nbefore. In essence, it records the latest timestamp of any record that has the\ntype attribute value \"a\".\n\n\nFinally, we can add a trigger on aggregates as follows:\n\n\nmlog\n-\nadd_trigger\n(\ntrigger_a\n,\n \nlastest_a \n 0\n);\n\n\n\n\n\n\nThis adds a trigger \"trigger_a\" on the aggregate \"latest_a\", which should \ngenerate an alert whenever the condition latest_a \n 0 is satisfied, i.e.,\nwhenever any record with  type = a is written to the Atomic MultiLog.\n\n\nWe are now ready to load some data into this multilog:\n\n\nsize_t\n \noff1\n \n=\n \nmlog\n-\nappend\n({\nb\n,\n \nHello World!\n);\n\n\nsize_t\n \noff2\n \n=\n \nmlog\n-\nappend\n({\nb\n,\n \nHow are you doing today?\n);\n\n\nsize_t\n \noff3\n \n=\n \nmlog\n-\nappend\n({\na\n,\n \nGood to see you!\n});\n\n\n\n\n\n\nNote that the \nappend\n method takes a vector of strings as its input, where\nthe vector corresponds to a single record. The number of entries in the\nvector must match the number of entries in the schema, with the exception\nof the timestamp --- if the timestamp is not provided, Confluo will automatically\nassign one.\n\n\nAlso note that the operation returns a unique offset corresponding to each append\noperation. This forms the \"key\" for records stored in the Atomic MultiLog -- records\ncan be retrieved by specifying their corresponding offsets.\n\n\nNow we take a look at how we can query the data in the Atomic MultiLog. First,\nit is straightforward to retrieve records given their offsets:\n\n\nauto\n \nrecord1\n \n=\n \nmlog\n-\nread\n(\noff1\n);\n\n\nauto\n \nrecord2\n \n=\n \nmlog\n-\nread\n(\noff2\n);\n\n\nauto\n \nrecord3\n \n=\n \nmlog\n-\nread\n(\noff3\n);\n\n\n\n\n\n\nEach of \nrecord1\n, \nrecord2\n, and \nrecord3\n are vectors of strings.\n\n\nWe can query indexed attributes as follows:\n\n\nauto\n \nrecord_stream1\n \n=\n \nmlog\n-\nexecute_filter\n(\ntype == b\n);\n\n\nfor\n \n(\nauto\n \ns\n \n=\n \nrecord_stream1\n;\n \n!\ns\n.\nempty\n();\n \ns\n \n=\n \ns\n.\ntail\n())\n \n{\n\n  \nstd\n::\ncout\n \n \ns\n.\nhead\n().\nto_string\n();\n\n\n}\n\n\n\n\n\n\nNote that the operation returns a lazily evaluated stream, which supports\nfunctional style operations like map, filter, etc. See \n\nstream.h\n\nfor more details.\n\n\nWe can also query the defined filter as follows:\n\n\nauto\n \nrecord_stream2\n \n=\n \nmlog\n-\nquery_filter\n(\ntype_a\n,\n \n0\n,\n \nUINT64_MAX\n);\n\n\nfor\n \n(\nauto\n \ns\n \n=\n \nrecord_stream2\n;\n \n!\ns\n.\nempty\n();\n \ns\n \n=\n \ns\n.\ntail\n())\n \n{\n\n  \nstd\n::\ncout\n \n \ns\n.\nhead\n().\nto_string\n();\n\n\n}\n\n\n\n\n\n\nThe first parameter corresponds to the name of the filter to be queried, while \nthe second and third parameters correspond to the begining timestamp and end \ntimestamp to consider for records in the filter. We've specified them to capture\nall possible values of timestamp. Similar to the \nexecute_filter\n query, this\noperation also returns a lazily evaluated record stream.\n\n\nWe query aggregates as follows:\n\n\nauto\n \nvalue\n \n=\n \nmlog\n-\nget_aggregate\n(\nlatest_a\n,\n \n0\n,\n \nUINT64_MAX\n);\n\n\nstd\n::\ncout\n \n \nvalue\n.\nto_string\n();\n\n\n\n\n\n\nThe query takes the name of the aggregate as its first parameter, while the \nsecond and third parameters correspond to begin and end timestmaps, as before.\nThe query returns a \nnumeric\n object,\nwhich is a wrapper around numeric values.\n\n\nFinally, we can query the generated alerts by triggers we have installed as \nfollows:\n\n\nauto\n \nalert_stream\n \n=\n \nmlog\n-\nget_alerts\n(\n0\n,\n \nUINT64_MAX\n,\n \ntrigger_a\n);\n\n\nfor\n \n(\nauto\n \ns\n \n=\n \nalert_stream\n;\n \n!\ns\n.\nempty\n();\n \ns\n \n=\n \ns\n.\ntail\n())\n \n{\n\n  \nstd\n::\ncout\n \n \ns\n.\nhead\n().\nto_string\n();\n\n\n}\n\n\n\n\n\n\nThe query takes and begin and end timestamps as its first and second arguments,\nand an optional trigger name as its third argument. The query returns a lazy \nstream over generated alerts for this trigger in the specified time-range.\n\n\nSee \nconfluo_store.h\n\nand \natomic_multilog.h\n\nto see all the APIs described here, along with variations and other caveats.\n\n\nStand-alone Mode\n\n\nIn the stand-alone mode, Confluo runs as a daemon server, serving client requests\nusing Apache Thrift protocol. To start the server, run:\n\n\nconfuod\n\n\n\n\n\nyou can find options supported by the binary using:\n\n\nconfluod --help\n\n\n\n\n\nOnce the server daemon is running, you can query it using the C++ or Python \nclient APIs. The client APIs closely resemble the embedded API, and can be\nfound \nhere\n \nfor the C++ Client, and \nhere\n \nfor the Python Client.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick_start/#quick-start", 
            "text": "In this Quick Start, we will take a look at how to download and setup Confluo,\nload some sample data, and query it.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick_start/#pre-requisites", 
            "text": "MacOS X or Unix-based OS; Windows is not yet supported.  C++ compiler that supports C++11 standard  CMake 2.8 or later  Boost 1.53 or later   For python client, you will additionally require:   Python 2.7 or later  Python Packages: six 1.7.2 or later   For java client, you will additionally require:   Java 1.7 or later  ant 1.6.2 or later", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/quick_start/#download-and-install", 
            "text": "To download and install Confluo, use the following commands:  git clone https://github.com/ucbrise/confluo.git cd  confluo\nmkdir build cd  build\ncmake ..\nmake -j   make  test    make install", 
            "title": "Download and Install"
        }, 
        {
            "location": "/quick_start/#using-confluo", 
            "text": "Confluo can be used in two modes -- embedded and stand-alone. In the embedded mode,\nConfluo is used as a header-only library in C++, allowing Confluo to use the same\naddress-space as the application process. In the stand-alone mode, Confluo runs\nas a daemon server process, allowing clients to communicate with it using  Apache Thrift  protocol.", 
            "title": "Using Confluo"
        }, 
        {
            "location": "/quick_start/#embedded-mode", 
            "text": "In order to use Confluo in the embedded mode, we simply need to include\nConfluo's header files under libconfluo/confluo, use the Confluo C++ API in \na C++ application, and compile using a modern C++ compiler. The entry point \nheader file to include is  confluo_store.h .   We will first create a new Confluo Store with the data path for it to use \nas follows:  confluo :: confluo_store   store ( /path/to/data );   We then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); this requires three parameters: name, schema, and the storage mode:  std :: string   schema ( { type: CHAR, msg: STRING(100) } );  auto   storage_mode   =   confluo :: storage :: IN_MEMORY ;  store . create_atomic_multilog ( my_multilog ,   schema ,   storage_mode );   Our simple schema only contains two attributes: a character type attribute, and \na string message field. Confluo will also automatically create a timestamp \nfield if the schema does not explicityly specify one.   We then obtain a reference to our newly created Atomic MultiLog:  confluo :: atomic_multilog *   mlog   =   store . get_atomic_multilog ( my_multilog );   We can define indexes on the Atomic MultiLog as follows:  mlog - add_index ( type )   to add an index on \"type\" attribute. We can also install filters as follows:  mlog - add_filter ( type_a ,   type == a );   to explicitly filter out records that have the \"type\" attribute value \"a\", using\na filter named \"type_a\".   Additionally, we can add aggregates on filters as follows:  mlog - add_aggregate ( latest_a ,   type_a ,   MAX(timestamp) )   This adds a new stored aggregate \"latest_a\" on the filter \"type_a\" we defined \nbefore. In essence, it records the latest timestamp of any record that has the\ntype attribute value \"a\".  Finally, we can add a trigger on aggregates as follows:  mlog - add_trigger ( trigger_a ,   lastest_a   0 );   This adds a trigger \"trigger_a\" on the aggregate \"latest_a\", which should \ngenerate an alert whenever the condition latest_a   0 is satisfied, i.e.,\nwhenever any record with  type = a is written to the Atomic MultiLog.  We are now ready to load some data into this multilog:  size_t   off1   =   mlog - append ({ b ,   Hello World! );  size_t   off2   =   mlog - append ({ b ,   How are you doing today? );  size_t   off3   =   mlog - append ({ a ,   Good to see you! });   Note that the  append  method takes a vector of strings as its input, where\nthe vector corresponds to a single record. The number of entries in the\nvector must match the number of entries in the schema, with the exception\nof the timestamp --- if the timestamp is not provided, Confluo will automatically\nassign one.  Also note that the operation returns a unique offset corresponding to each append\noperation. This forms the \"key\" for records stored in the Atomic MultiLog -- records\ncan be retrieved by specifying their corresponding offsets.  Now we take a look at how we can query the data in the Atomic MultiLog. First,\nit is straightforward to retrieve records given their offsets:  auto   record1   =   mlog - read ( off1 );  auto   record2   =   mlog - read ( off2 );  auto   record3   =   mlog - read ( off3 );   Each of  record1 ,  record2 , and  record3  are vectors of strings.  We can query indexed attributes as follows:  auto   record_stream1   =   mlog - execute_filter ( type == b );  for   ( auto   s   =   record_stream1 ;   ! s . empty ();   s   =   s . tail ())   { \n   std :: cout     s . head (). to_string ();  }   Note that the operation returns a lazily evaluated stream, which supports\nfunctional style operations like map, filter, etc. See  stream.h \nfor more details.  We can also query the defined filter as follows:  auto   record_stream2   =   mlog - query_filter ( type_a ,   0 ,   UINT64_MAX );  for   ( auto   s   =   record_stream2 ;   ! s . empty ();   s   =   s . tail ())   { \n   std :: cout     s . head (). to_string ();  }   The first parameter corresponds to the name of the filter to be queried, while \nthe second and third parameters correspond to the begining timestamp and end \ntimestamp to consider for records in the filter. We've specified them to capture\nall possible values of timestamp. Similar to the  execute_filter  query, this\noperation also returns a lazily evaluated record stream.  We query aggregates as follows:  auto   value   =   mlog - get_aggregate ( latest_a ,   0 ,   UINT64_MAX );  std :: cout     value . to_string ();   The query takes the name of the aggregate as its first parameter, while the \nsecond and third parameters correspond to begin and end timestmaps, as before.\nThe query returns a  numeric  object,\nwhich is a wrapper around numeric values.  Finally, we can query the generated alerts by triggers we have installed as \nfollows:  auto   alert_stream   =   mlog - get_alerts ( 0 ,   UINT64_MAX ,   trigger_a );  for   ( auto   s   =   alert_stream ;   ! s . empty ();   s   =   s . tail ())   { \n   std :: cout     s . head (). to_string ();  }   The query takes and begin and end timestamps as its first and second arguments,\nand an optional trigger name as its third argument. The query returns a lazy \nstream over generated alerts for this trigger in the specified time-range.  See  confluo_store.h \nand  atomic_multilog.h \nto see all the APIs described here, along with variations and other caveats.", 
            "title": "Embedded Mode"
        }, 
        {
            "location": "/quick_start/#stand-alone-mode", 
            "text": "In the stand-alone mode, Confluo runs as a daemon server, serving client requests\nusing Apache Thrift protocol. To start the server, run:  confuod  you can find options supported by the binary using:  confluod --help  Once the server daemon is running, you can query it using the C++ or Python \nclient APIs. The client APIs closely resemble the embedded API, and can be\nfound  here  \nfor the C++ Client, and  here  \nfor the Python Client.", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/install/", 
            "text": "Installation\n\n\nBefore you can install Confluo, make sure you have the following prerequisites:\n\n\n\n\nMacOS X or Unix-based OS; Windows is not yet supported.\n\n\nC++ compiler that supports C++11 standard\n\n\nCMake 2.8 or later\n\n\nBoost 1.53 or later\n\n\n\n\nFor python client, you will additionally require:\n\n\n\n\nPython 2.7 or later\n\n\nPython Packages: six 1.7.2 or later\n\n\n\n\nFor java client, you will additionally require:\n\n\n\n\nJava 1.7 or later\n\n\nant 1.6.2 or later\n\n\n\n\nDownload\n\n\nYou can obtain the latest version of Confluo by cloning the GitHub repository:\n\n\ngit clone https://github.com/ucbrise/confluo.git\n\n\n\n\n\nConfigure\n\n\nTo configure the build, Confluo uses CMake as its build system. Confluo only \nsupports out of source builds; the simplest way to configure the build would be \nas follows:\n\n\ncd\n confluo\nmkdir -p build\n\ncd\n build\ncmake ..\n\n\n\n\n\nIt is possible to configure the build specifying certain options based on \nrequirements; the supported options are:\n\n\n\n\nBUILD_TESTS\n: Builds all tests (ON by default)\n\n\nBUILD_RPC\n: Builds the rpc daemon and client libraries (ON by default)\n\n\nBUILD_EXAMPLES\n: Builds Confluo examples (ON by default)\n\n\nBUILD_DOC\n: Builds Confluo documentation (OFF by default)\n\n\nWITH_PY_CLIENT\n: Builds Confluo python rpc client (ON by default)\n\n\nWITH_JAVA_CLIENT\n: Builds Confluo java rpc client (ON by default)\n\n\n\n\nIn order to explicitly enable or disable any of these options, set the value of\nthe corresponding variable to \nON\n or \nOFF\n as follows:\n\n\ncmake -DBUILD_TESTS\n=\nOFF\n\n\n\n\n\nFinally, you can configure the install location for Confluo by modifying the\n\nCMAKE_INSTALL_PREFIX\n variable (which is set to /usr/local by default):\n\n\ncmake -DCMAKE_INSTALL_PREFIX\n=\n/path/to/installation\n\n\n\n\n\nInstall\n\n\nOnce the build is configured, you can proceed to compile, test and install \nConfluo. \n\n\nTo build, use:\n\n\nmake\n\n\n\n\n\nor \n\n\nmake -j\n{\nNUM_CORES\n}\n\n\n\n\n\n\nto speed up the build on multi-core systems.\n\n\nTo run the various unit tests, run:\n\n\nmake \ntest\n\n\n\n\n\n\nand finally, to install, use:\n\n\nmake install", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installation", 
            "text": "Before you can install Confluo, make sure you have the following prerequisites:   MacOS X or Unix-based OS; Windows is not yet supported.  C++ compiler that supports C++11 standard  CMake 2.8 or later  Boost 1.53 or later   For python client, you will additionally require:   Python 2.7 or later  Python Packages: six 1.7.2 or later   For java client, you will additionally require:   Java 1.7 or later  ant 1.6.2 or later", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#download", 
            "text": "You can obtain the latest version of Confluo by cloning the GitHub repository:  git clone https://github.com/ucbrise/confluo.git", 
            "title": "Download"
        }, 
        {
            "location": "/install/#configure", 
            "text": "To configure the build, Confluo uses CMake as its build system. Confluo only \nsupports out of source builds; the simplest way to configure the build would be \nas follows:  cd  confluo\nmkdir -p build cd  build\ncmake ..  It is possible to configure the build specifying certain options based on \nrequirements; the supported options are:   BUILD_TESTS : Builds all tests (ON by default)  BUILD_RPC : Builds the rpc daemon and client libraries (ON by default)  BUILD_EXAMPLES : Builds Confluo examples (ON by default)  BUILD_DOC : Builds Confluo documentation (OFF by default)  WITH_PY_CLIENT : Builds Confluo python rpc client (ON by default)  WITH_JAVA_CLIENT : Builds Confluo java rpc client (ON by default)   In order to explicitly enable or disable any of these options, set the value of\nthe corresponding variable to  ON  or  OFF  as follows:  cmake -DBUILD_TESTS = OFF  Finally, you can configure the install location for Confluo by modifying the CMAKE_INSTALL_PREFIX  variable (which is set to /usr/local by default):  cmake -DCMAKE_INSTALL_PREFIX = /path/to/installation", 
            "title": "Configure"
        }, 
        {
            "location": "/install/#install", 
            "text": "Once the build is configured, you can proceed to compile, test and install \nConfluo.   To build, use:  make  or   make -j { NUM_CORES }   to speed up the build on multi-core systems.  To run the various unit tests, run:  make  test   and finally, to install, use:  make install", 
            "title": "Install"
        }, 
        {
            "location": "/modes_of_operation/", 
            "text": "Modes of Operation\n\n\nConfluo can be used in two modes -- embedded and stand-alone. \n\n\nEmbedded Mode\n\n\nIn the \nembedded mode\n, Confluo is used as a header-only library in C++, allowing Confluo\nto use the same address-space as the application process. This enables ultra low-latency \nwrites and queries, but only supports applications written in C++.\n\n\nStand-alone Mode\n\n\nConfluo also supports a \nstand-alone mode\n, where Confluo runs as a daemon \nserver process and allows clients to communicate with it using \n\nApache Thrift\n protocol. Operations now incur higher latencies\n(due to serialization/deserialization overheads), but can now operate over the network, \nand allows Confluo to store data from applications written in different languages.\n\n\nMore on Usage\n\n\nRead more on how you can perform different operations with the two modes of operation:\n\n\n\n\nData Storage and Loading Data\n\n\nQuerying Data\n\n\nOnline Queries\n\n\nOffline Queries", 
            "title": "Modes of Operation"
        }, 
        {
            "location": "/modes_of_operation/#modes-of-operation", 
            "text": "Confluo can be used in two modes -- embedded and stand-alone.", 
            "title": "Modes of Operation"
        }, 
        {
            "location": "/modes_of_operation/#embedded-mode", 
            "text": "In the  embedded mode , Confluo is used as a header-only library in C++, allowing Confluo\nto use the same address-space as the application process. This enables ultra low-latency \nwrites and queries, but only supports applications written in C++.", 
            "title": "Embedded Mode"
        }, 
        {
            "location": "/modes_of_operation/#stand-alone-mode", 
            "text": "Confluo also supports a  stand-alone mode , where Confluo runs as a daemon \nserver process and allows clients to communicate with it using  Apache Thrift  protocol. Operations now incur higher latencies\n(due to serialization/deserialization overheads), but can now operate over the network, \nand allows Confluo to store data from applications written in different languages.", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/modes_of_operation/#more-on-usage", 
            "text": "Read more on how you can perform different operations with the two modes of operation:   Data Storage and Loading Data  Querying Data  Online Queries  Offline Queries", 
            "title": "More on Usage"
        }, 
        {
            "location": "/loading_data/", 
            "text": "Data Storage and Loading\n\n\nConfluo operates on \ndata streams\n. Each stream comprises of records, each of\nwhich follows a pre-defined schema over a collection of strongly-typed \nattributes.\n\n\nAttributes\n\n\nConfluo currently supports only \nbounded-width\n attributes\n1\n. An attribute \nis said to have bounded-width if all records in a single stream use some \nmaximum number of bits to represent that attribute; this includes primitive \ndata types such as binary, integral or floating-point values, or \ndomain-specific types such as IP addresses, ports, sensor readings, etc. \nConfluo also requires each record in the stream to have a 8-byte nano-second \nprecision timestamp attribute; if the application does not assign timestamps, \nConfluo internally assigns one during the write operation. \n\n\nSchema\n\n\nA schema in Confluo is a collection of strongly-typed attributes. It is specified\nvia JSON like semantics; for instance, consider the example below for a simple schema \nwith three attributes: \n\n\n{\n\n  \ntimestamp:\n \nLONG,\n\n  \nop_latency_ms:\n \nDOUBLE,\n\n  \ncpu_util:\n \nDOUBLE,\n\n  \nmem_avail:\n \nDOUBLE,\n\n  \nlog_msg:\n \nSTRING(100)\n\n\n}\n\n\n\n\n\n\nThe first attribute is timestamp with a 8-byte signed integer type; the second, third and\nfourth attributes correspond to operation latency (in ms), CPU utilization and available \nmemory respectively, all with double precision floating-point type. The final attribute is a\nlog message, with a string type upper bound by 100 characters. Note that each of the \nattributes must have types associated with them, and each record in a\nstream with this schema must have its attributes in this order. While Confluo natively \nsupports common primitive types, you can add custom bounded-width data types to Confluo's\ntype system. More details can be found at the \nConfluo Type-System\n guide.\n\n\nAtomic MultiLog\n\n\nAtomic MultiLogs are the basic storage abstraction in Confluo, and are similar in\ninterface to database tables. In order to store data from different streams, \napplications can create an Atomic MultiLog with a pre-specified schema, \nand write data streams that conform to the schema to the Atomic MultiLog.\n\n\nTo support queries, applications can add an index for individual attributes in the schema. \nConfluo also employs a match-action language with three main elements: \nfilter\n, \n\naggregate\n  and \ntrigger\n. \n\n\n\n\nA Confluo filter is an expression comprising of relational and boolean operators \n(see Table below) over arbitrary subset of bounded-width attributes, and identifies records \nthat match the expression. \n\n\nA Confluo aggregate evaluates a computable function on an attribute for all records that \nmatch a certain filter expression. \n\n\nFinally, a Confluo trigger is a boolean conditional (e.g., \n, \n, =, etc.) evaluated over \na Confluo aggregate. \n\n\n\n\nRelational Operators in Filters\n:\n\n\n\n\n\n\n\n\nOperator\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nEquality\n\n\ndst_port=80\n\n\n\n\n\n\nRange\n\n\ncpu_util\n0.8\n\n\n\n\n\n\n\n\nBoolean Operators in Filters\n:\n\n\n\n\n\n\n\n\nOperator\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nConjunction\n\n\nvolt\n200 \n temp\n100\n\n\n\n\n\n\nDisjunction\n\n\ncpu_util\n0.8 || mem_avail\n0.1\n\n\n\n\n\n\nNegation\n\n\ntransport_protocol != TCP\n\n\n\n\n\n\n\n\nConfluo supports indexes, filters, aggregates and triggers only on bounded-width\nattributes in the schema. Once added, each of these are evaluated and updated \nupon arrival of each new batch of data records.\n\n\nLoading Data: A Performance Monitoring and Diagnosis Example\n\n\nWe will now see how we can create Atomic MultiLogs, add indexes, filters, aggregate and triggers on them,\nand finally load some data into them, for both embedded and stand-alone modes of operation. We will work\nwith the example of a performance monitoring and diagnosis tool using Confluo.\n\n\nEmbedded mode\n\n\nIn order to use Confluo in the embedded mode, we simply need to include\nConfluo's header files under libconfluo/confluo, use the Confluo C++ API in \na C++ application, and compile using a modern C++ compiler. The entry point \nheader file to include is \nconfluo_store.h\n. \n\n\nCreating a New Confluo Store\n\n\nWe will first create a new Confluo Store with the data path for it to use \nas follows:\n\n\nconfluo\n::\nconfluo_store\n \nstore\n(\n/path/to/data\n);\n\n\n\n\n\n\nCreating a New Atomic MultiLog\n\n\nWe then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); this requires three parameters: a name for the Atomic MultiLog, a fixed\nschema, and a storage mode:\n\n\nstd\n::\nstring\n \nschema\n \n=\n \n{\n\n  \ntimestamp\n:\n \nLONG\n,\n\n  \nop_latency_ms\n:\n \nDOUBLE\n,\n\n  \ncpu_util\n:\n \nDOUBLE\n,\n\n  \nmem_avail\n:\n \nDOUBLE\n,\n\n  \nlog_msg\n:\n \nSTRING\n(\n100\n)\n\n\n}\n;\n\n\nauto\n \nstorage_mode\n \n=\n \nconfluo\n::\nstorage\n::\nIN_MEMORY\n;\n\n\nstore\n.\ncreate_atomic_multilog\n(\nperf_log\n,\n \nschema\n,\n \nstorage_mode\n);\n\n\n\n\n\n\nOur Atomic MultiLog adopts the same schema outlined \nabove\n. The \nstorage mode is set to in-memory, but can be of the following types:\n\n\n\n\n\n\n\n\nStorage Mode\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nIN_MEMORY\n\n\nAll data is written purely in memory, and no attempt is made at persisting data to secondary storage.\n\n\n\n\n\n\nDURABLE\n\n\nOnly the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage for each write. The write is not considered complete unless its effects have been persisted to secondary storage.\n\n\n\n\n\n\nDURABLE_RELAXED\n\n\nOnly the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage; however, the data is buffered in memory and only persisted periodically, instead of persisting data for every write. This generally leads to better write performance.\n\n\n\n\n\n\n\n\nWe then obtain a reference to our newly created Atomic MultiLog:\n\n\nconfluo\n::\natomic_multilog\n*\n \nmlog\n \n=\n \nstore\n.\nget_atomic_multilog\n(\nperf_log\n);\n\n\n\n\n\n\nAdding Indexes\n\n\nWe can define indexes on the Atomic MultiLog as follows:\n\n\nmlog\n-\nadd_index\n(\nop_latency_ms\n);\n\n\n\n\n\n\nto add an index on \nop_latency_ms\n attribute. \n\n\nAdding Filters\n\n\nWe can also install filters as follows:\n\n\nmlog\n-\nadd_filter\n(\nlow_resources\n,\n \ncpu_util\n0.8 || mem_avail\n0.1\n);\n\n\n\n\n\n\nto explicitly filter out records that indicate low system resources (CPU \nutilization \n 80%, Available Memory \n 10%), using a filter named \nlow_resources\n. \n\n\nAdding Aggregates\n\n\nAdditionally, we can add aggregates on filters as follows:\n\n\nmlog\n-\nadd_aggregate\n(\nmax_latency_ms\n,\n \nlow_resources\n,\n \nMAX(op_latency_ms)\n);\n\n\n\n\n\n\nThis adds a new stored aggregate \nmax_latency_ms\n on the filter \n\nlow_resources\n we defined before. In essence, it records the highest \noperation latency reported in any record that also indicated low \navailable resources.\n\n\nInstalling Triggers\n\n\nFinally, we can install a trigger on aggregates as follows:\n\n\nmlog\n-\ninstall_trigger\n(\nhigh_latency_trigger\n,\n \nmax_latency \n 1000\n);\n\n\n\n\n\n\nThis installs a trigger \nhigh_latency_trigger\n on the aggregate \n\nmax_latency_ms\n, which should generate an alert whenever the \ncondition \nmax_latency_ms \n 1000\n is satisfied, i.e.,\nwhenever the maximum latency for an operation exceeds 1s and\nthe available resources are low.\n\n\nLoading sample data into Atomic MultiLog\n\n\nWe are now ready to load some data into this Atomic MultiLog. Atomic MutliLogs\nonly support addition of new data via \nappends\n. However, new data can be\nappended in several ways:\n\n\nAppending String Vectors\n\n\nThis version of \nappend\n method takes a vector of strings as its input, where\nthe vector corresponds to a single record. The number of entries in the\nvector must match the number of entries in the schema, with the exception\nof the timestamp --- if the timestamp is not provided, Confluo will automatically\nassign one.\n\n\nsize_t\n \noff1\n \n=\n \nmlog\n-\nappend\n({\n100\n,\n \n0.5\n,\n \n0.9\n,\n  \nINFO: Launched 1 tasks\n});\n\n\nsize_t\n \noff2\n \n=\n \nmlog\n-\nappend\n({\n500\n,\n \n0.9\n,\n \n0.05\n,\n \nWARN: Server {2} down\n});\n\n\nsize_t\n \noff3\n \n=\n \nmlog\n-\nappend\n({\n1001\n,\n \n0.9\n,\n \n0.03\n,\n \nWARN: Server {2, 4, 5} down\n});\n\n\n\n\n\n\nAlso note that the operation returns a unique offset corresponding to each append\noperation. This forms the \"key\" for records stored in the Atomic MultiLog -- records\ncan be retrieved by specifying their corresponding offsets.\n\n\nAppending Raw bytes\n\n\nThis version of \nappend\n takes as its input a pointer to a C/C++ struct, that maps\nexactly to the Atomic MultiLog's schema. For instance, our schema would map to the following\nC/C++ struct:\n\n\nstruct\n \nperf_log_record\n \n{\n\n  \nint64_t\n \ntimestamp\n;\n\n  \ndouble\n \nop_latency_ms\n;\n\n  \ndouble\n \ncpu_util\n;\n\n  \ndouble\n \nmem_avail\n;\n\n  \nchar\n \nlog_msg\n[\n100\n];\n\n\n};\n\n\n\n\n\n\nNote that \nlog_msg\n maps to a \nchar[100]\n rather than an \nstd::string\n. To add a new record, we\nwould populate a struct instance, and pass its reference to the append function:\n\n\nint64_t\n \nts\n \n=\n \nutils\n::\ntime_utils\n::\ncur_ns\n();\n\n\nperf_log_record\n \nrec\n \n=\n \n{\n \nts\n,\n \n2000.0\n,\n \n0.95\n,\n \n0.01\n,\n \nWARN: Server {2, 4, 5} down\n \n};\n\n\nsize_t\n \noff4\n \n=\n \nmlog\n-\nappend\n(\nrec\n);\n\n\n\n\n\n\nNote that this is a more efficient variant of append, since it avoids the overheads of parsing \nstrings to the corresponding attribute data types.\n\n\nBatched Appends\n\n\nIt is also possible to batch multiple record appends into a single append. The first\nstep in building a batch is to obtain a batch builder:\n\n\nauto\n \nbatch_bldr\n \n=\n \nmlog\n-\nget_batch_builder\n();\n\n\n\n\n\n\nThe batch builder supports adding new records via both string vector and raw byte interfaces:\n\n\nbatch_bldr\n.\nadd_record\n({\n \n400\n,\n \n0.85\n,\n \n0.07\n,\n \nWARN: Server {2, 4} down\n});\n\n\nperf_log_record\n \nrec\n \n=\n \n{\n \nutils\n::\ntime_utils\n::\ncur_ns\n(),\n \n100.0\n,\n \n0.65\n,\n \n0.25\n,\n \nWARN: Server {2} down\n \n};\n\n\nbatch_bldr\n.\nadd_record\n(\nrec\n);\n\n\n\n\n\n\nOnce the batch is populated, we can append the batch to the Atomic MultiLog as follows:\n\n\nsize_t\n \noff5\n \n=\n \nmlog\n-\nappend_batch\n(\nbatch_bldr\n.\nget_batch\n());\n\n\n\n\n\n\nTo understand how we can query the data we have loaded so far, read the guide on \nConfluo Queries\n.\n\n\nStand-alone Mode\n\n\nIn the stand-alone mode, Confluo runs as a daemon server, serving client requests\nusing Apache Thrift protocol. To start the server, run:\n\n\nconfuod --address\n=\n127\n.0.0.1 --port\n=\n9090\n\n\n\n\n\n\nOnce the server daemon is running, you can send requests to it using the \nC++/Python/Java client APIs. We will focus on the C++ Client API, although \nthe Python/Java API is almost identical. In fact, even the C++ Client API is \nalmost identical to the embedded mode API.\n\n\nWe look at the same performance monitoring and diagnosis tool example for the\nstand-alone mode. The relevant header file to include for the C++ Client API is\n\nrpc_client.h\n.\n\n\nCreating a Client Connection\n\n\nTo begin with, we first have to establish a client connection with the server.\n\n\nconfluo\n::\nrpc\n::\nrpc_client\n \nclient\n(\n127.0.0.1\n,\n \n9090\n);\n\n\n\n\n\n\nThe first argument to the \nrpc_client\n constructor corresponds to the server\nhostname, while the second argument corresponds to the server port.\n\n\nCreating a New Atomic MultiLog\n\n\nWe then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); as before, this requires three parameters: a name for the Atomic \nMultiLog, a fixed schema, and a storage mode:\n\n\nstd\n::\nstring\n \nschema\n \n=\n \n{\n\n  \ntimestamp\n:\n \nLONG\n,\n\n  \nop_latency_ms\n:\n \nDOUBLE\n,\n\n  \ncpu_util\n:\n \nDOUBLE\n,\n\n  \nmem_avail\n:\n \nDOUBLE\n,\n\n  \nlog_msg\n:\n \nSTRING\n(\n100\n)\n\n\n}\n;\n\n\nauto\n \nstorage_mode\n \n=\n \nconfluo\n::\nstorage\n::\nIN_MEMORY\n;\n\n\nclient\n.\ncreate_atomic_multilog\n(\nperf_log\n,\n \nschema\n,\n \nstorage_mode\n);\n\n\n\n\n\n\nThis operation also internally sets the current Atomic MultiLog \nfor the client to the one we just created (i.e., \nperf_log\n). It\nis also possible to explicitly set the current Atomic MultiLog for\nthe client as follows:\n\n\nclient\n.\nset_current_atomic_multilog\n(\nperf_log\n);\n\n\n\n\n\n\n\n\nNote\n\n\nIt is necessary to set the current Atomic MultiLog for the \nrpc_client\n;\nissuing requests via the client without setting the current Atomic MultiLog\nwill result in exceptions.\n\n\n\n\nAdding Indexes\n\n\nWe can define indexes as follows:\n\n\nclient\n.\nadd_index\n(\nop_latency_ms\n);\n\n\n\n\n\n\nAdding Filters\n\n\nWe can also install filters as follows:\n\n\nclient\n.\nadd_filter\n(\nlow_resources\n,\n \ncpu_util\n0.8 || mem_avail\n0.1\n);\n\n\n\n\n\n\nAdding Aggregates\n\n\nAdditionally, we can add aggregates on filters as follows:\n\n\nclient\n.\nadd_aggregate\n(\nmax_latency_ms\n,\n \nlow_resources\n,\n \nMAX(op_latency_ms)\n);\n\n\n\n\n\n\nInstalling Triggers\n\n\nFinally, we can install a trigger on an aggregate as follows:\n\n\nclient\n.\ninstall_trigger\n(\nhigh_latency_trigger\n,\n \nmax_latency \n 1000\n);\n\n\n\n\n\n\nLoading sample data into Atomic MultiLog\n\n\nWe are now ready to load some data into the Atomic MultiLog on the server. \n\n\nAppending String Vectors\n\n\nsize_t\n \noff1\n \n=\n \nclient\n.\nappend\n({\n100\n,\n \n0.5\n,\n \n0.9\n,\n  \nINFO: Launched 1 tasks\n});\n\n\nsize_t\n \noff2\n \n=\n \nclient\n.\nappend\n({\n500\n,\n \n0.9\n,\n \n0.05\n,\n \nWARN: Server {2} down\n});\n\n\nsize_t\n \noff3\n \n=\n \nclient\n.\nappend\n({\n1001\n,\n \n0.9\n,\n \n0.03\n,\n \nWARN: Server {2, 4, 5} down\n});\n\n\n\n\n\n\nAppending Raw bytes\n\n\nAs with the embedded mode, this version of \nappend\n takes as its input a pointer to a C/C++ struct that maps\n\nexactly\n to the Atomic MultiLog's schema. Our schema would map to the following C/C++ struct:\n\n\nstruct\n \nperf_log_record\n \n{\n\n  \nint64_t\n \ntimestamp\n;\n\n  \ndouble\n \nop_latency_ms\n;\n\n  \ndouble\n \ncpu_util\n;\n\n  \ndouble\n \nmem_avail\n;\n\n  \nchar\n \nlog_msg\n[\n100\n];\n\n\n};\n\n\n\n\n\n\nUnlike the embedded interface, to add a new record, we now have to wrap the struct reference in a \nrecord_data\n object:\n\n\nint64_t\n \nts\n \n=\n \nutils\n::\ntime_utils\n::\ncur_ns\n();\n\n\nperf_log_record\n \nrec\n \n=\n \n{\n \nts\n,\n \n2000.0\n,\n \n0.95\n,\n \n0.01\n,\n \nWARN: Server {2, 4, 5} down\n \n};\n\n\nsize_t\n \noff4\n \n=\n \nclient\n.\nappend\n(\nconfluo\n::\nrpc\n::\nrecord_data\n(\nrec\n,\n \nsizeof\n(\nrec\n)));\n\n\n\n\n\n\nAs before, this is a more efficient variant of append, since it avoids the overheads of parsing \nstrings to the corresponding attribute data types.\n\n\nBatched Appends\n\n\nIt is also possible to batch multiple record appends into a single append operation via the client API. \nThis is particularly useful since batching helps amortize the cost of network latency.\n\n\nThe first step in building a batch is to obtain a batch builder:\n\n\nauto\n \nbatch_bldr\n \n=\n \nclient\n.\nget_batch_builder\n();\n\n\n\n\n\n\nThe batch builder supports adding new records via both string vector and raw byte interfaces:\n\n\nbatch_bldr\n.\nadd_record\n({\n \n400\n,\n \n0.85\n,\n \n0.07\n,\n \nWARN: Server {2, 4} down\n});\n\n\nperf_log_record\n \nrec\n \n=\n \n{\n \nutils\n::\ntime_utils\n::\ncur_ns\n(),\n \n100.0\n,\n \n0.65\n,\n \n0.25\n,\n \nWARN: Server {2} down\n \n};\n\n\nbatch_bldr\n.\nadd_record\n(\nconfluo\n::\nrpc\n::\nrecord_data\n(\nrec\n,\n \nsizeof\n(\nrec\n)));\n\n\n\n\n\n\nOnce the batch is populated, we can append the batch as follows:\n\n\nsize_t\n \noff5\n \n=\n \nclient\n.\nappend_batch\n(\nbatch_bldr\n.\nget_batch\n());\n\n\n\n\n\n\nDetails on querying the data via the client interface can be found in the guide on \nConfluo Queries\n.\n\n\n\n\n\n\n\n\n\n\nWe plan on adding support for variable width data types in a future relase.", 
            "title": "Data Storage Overview"
        }, 
        {
            "location": "/loading_data/#data-storage-and-loading", 
            "text": "Confluo operates on  data streams . Each stream comprises of records, each of\nwhich follows a pre-defined schema over a collection of strongly-typed \nattributes.", 
            "title": "Data Storage and Loading"
        }, 
        {
            "location": "/loading_data/#attributes", 
            "text": "Confluo currently supports only  bounded-width  attributes 1 . An attribute \nis said to have bounded-width if all records in a single stream use some \nmaximum number of bits to represent that attribute; this includes primitive \ndata types such as binary, integral or floating-point values, or \ndomain-specific types such as IP addresses, ports, sensor readings, etc. \nConfluo also requires each record in the stream to have a 8-byte nano-second \nprecision timestamp attribute; if the application does not assign timestamps, \nConfluo internally assigns one during the write operation.", 
            "title": "Attributes"
        }, 
        {
            "location": "/loading_data/#schema", 
            "text": "A schema in Confluo is a collection of strongly-typed attributes. It is specified\nvia JSON like semantics; for instance, consider the example below for a simple schema \nwith three attributes:   { \n   timestamp:   LONG, \n   op_latency_ms:   DOUBLE, \n   cpu_util:   DOUBLE, \n   mem_avail:   DOUBLE, \n   log_msg:   STRING(100)  }   The first attribute is timestamp with a 8-byte signed integer type; the second, third and\nfourth attributes correspond to operation latency (in ms), CPU utilization and available \nmemory respectively, all with double precision floating-point type. The final attribute is a\nlog message, with a string type upper bound by 100 characters. Note that each of the \nattributes must have types associated with them, and each record in a\nstream with this schema must have its attributes in this order. While Confluo natively \nsupports common primitive types, you can add custom bounded-width data types to Confluo's\ntype system. More details can be found at the  Confluo Type-System  guide.", 
            "title": "Schema"
        }, 
        {
            "location": "/loading_data/#atomic-multilog", 
            "text": "Atomic MultiLogs are the basic storage abstraction in Confluo, and are similar in\ninterface to database tables. In order to store data from different streams, \napplications can create an Atomic MultiLog with a pre-specified schema, \nand write data streams that conform to the schema to the Atomic MultiLog.  To support queries, applications can add an index for individual attributes in the schema. \nConfluo also employs a match-action language with three main elements:  filter ,  aggregate   and  trigger .    A Confluo filter is an expression comprising of relational and boolean operators \n(see Table below) over arbitrary subset of bounded-width attributes, and identifies records \nthat match the expression.   A Confluo aggregate evaluates a computable function on an attribute for all records that \nmatch a certain filter expression.   Finally, a Confluo trigger is a boolean conditional (e.g.,  ,  , =, etc.) evaluated over \na Confluo aggregate.    Relational Operators in Filters :     Operator  Examples      Equality  dst_port=80    Range  cpu_util 0.8     Boolean Operators in Filters :     Operator  Examples      Conjunction  volt 200   temp 100    Disjunction  cpu_util 0.8 || mem_avail 0.1    Negation  transport_protocol != TCP     Confluo supports indexes, filters, aggregates and triggers only on bounded-width\nattributes in the schema. Once added, each of these are evaluated and updated \nupon arrival of each new batch of data records.", 
            "title": "Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#loading-data-a-performance-monitoring-and-diagnosis-example", 
            "text": "We will now see how we can create Atomic MultiLogs, add indexes, filters, aggregate and triggers on them,\nand finally load some data into them, for both embedded and stand-alone modes of operation. We will work\nwith the example of a performance monitoring and diagnosis tool using Confluo.", 
            "title": "Loading Data: A Performance Monitoring and Diagnosis Example"
        }, 
        {
            "location": "/loading_data/#embedded-mode", 
            "text": "In order to use Confluo in the embedded mode, we simply need to include\nConfluo's header files under libconfluo/confluo, use the Confluo C++ API in \na C++ application, and compile using a modern C++ compiler. The entry point \nheader file to include is  confluo_store.h .", 
            "title": "Embedded mode"
        }, 
        {
            "location": "/loading_data/#creating-a-new-confluo-store", 
            "text": "We will first create a new Confluo Store with the data path for it to use \nas follows:  confluo :: confluo_store   store ( /path/to/data );", 
            "title": "Creating a New Confluo Store"
        }, 
        {
            "location": "/loading_data/#creating-a-new-atomic-multilog", 
            "text": "We then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); this requires three parameters: a name for the Atomic MultiLog, a fixed\nschema, and a storage mode:  std :: string   schema   =   { \n   timestamp :   LONG , \n   op_latency_ms :   DOUBLE , \n   cpu_util :   DOUBLE , \n   mem_avail :   DOUBLE , \n   log_msg :   STRING ( 100 )  } ;  auto   storage_mode   =   confluo :: storage :: IN_MEMORY ;  store . create_atomic_multilog ( perf_log ,   schema ,   storage_mode );   Our Atomic MultiLog adopts the same schema outlined  above . The \nstorage mode is set to in-memory, but can be of the following types:     Storage Mode  Description      IN_MEMORY  All data is written purely in memory, and no attempt is made at persisting data to secondary storage.    DURABLE  Only the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage for each write. The write is not considered complete unless its effects have been persisted to secondary storage.    DURABLE_RELAXED  Only the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage; however, the data is buffered in memory and only persisted periodically, instead of persisting data for every write. This generally leads to better write performance.     We then obtain a reference to our newly created Atomic MultiLog:  confluo :: atomic_multilog *   mlog   =   store . get_atomic_multilog ( perf_log );", 
            "title": "Creating a New Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#adding-indexes", 
            "text": "We can define indexes on the Atomic MultiLog as follows:  mlog - add_index ( op_latency_ms );   to add an index on  op_latency_ms  attribute.", 
            "title": "Adding Indexes"
        }, 
        {
            "location": "/loading_data/#adding-filters", 
            "text": "We can also install filters as follows:  mlog - add_filter ( low_resources ,   cpu_util 0.8 || mem_avail 0.1 );   to explicitly filter out records that indicate low system resources (CPU \nutilization   80%, Available Memory   10%), using a filter named  low_resources .", 
            "title": "Adding Filters"
        }, 
        {
            "location": "/loading_data/#adding-aggregates", 
            "text": "Additionally, we can add aggregates on filters as follows:  mlog - add_aggregate ( max_latency_ms ,   low_resources ,   MAX(op_latency_ms) );   This adds a new stored aggregate  max_latency_ms  on the filter  low_resources  we defined before. In essence, it records the highest \noperation latency reported in any record that also indicated low \navailable resources.", 
            "title": "Adding Aggregates"
        }, 
        {
            "location": "/loading_data/#installing-triggers", 
            "text": "Finally, we can install a trigger on aggregates as follows:  mlog - install_trigger ( high_latency_trigger ,   max_latency   1000 );   This installs a trigger  high_latency_trigger  on the aggregate  max_latency_ms , which should generate an alert whenever the \ncondition  max_latency_ms   1000  is satisfied, i.e.,\nwhenever the maximum latency for an operation exceeds 1s and\nthe available resources are low.", 
            "title": "Installing Triggers"
        }, 
        {
            "location": "/loading_data/#loading-sample-data-into-atomic-multilog", 
            "text": "We are now ready to load some data into this Atomic MultiLog. Atomic MutliLogs\nonly support addition of new data via  appends . However, new data can be\nappended in several ways:", 
            "title": "Loading sample data into Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#appending-string-vectors", 
            "text": "This version of  append  method takes a vector of strings as its input, where\nthe vector corresponds to a single record. The number of entries in the\nvector must match the number of entries in the schema, with the exception\nof the timestamp --- if the timestamp is not provided, Confluo will automatically\nassign one.  size_t   off1   =   mlog - append ({ 100 ,   0.5 ,   0.9 ,    INFO: Launched 1 tasks });  size_t   off2   =   mlog - append ({ 500 ,   0.9 ,   0.05 ,   WARN: Server {2} down });  size_t   off3   =   mlog - append ({ 1001 ,   0.9 ,   0.03 ,   WARN: Server {2, 4, 5} down });   Also note that the operation returns a unique offset corresponding to each append\noperation. This forms the \"key\" for records stored in the Atomic MultiLog -- records\ncan be retrieved by specifying their corresponding offsets.", 
            "title": "Appending String Vectors"
        }, 
        {
            "location": "/loading_data/#appending-raw-bytes", 
            "text": "This version of  append  takes as its input a pointer to a C/C++ struct, that maps\nexactly to the Atomic MultiLog's schema. For instance, our schema would map to the following\nC/C++ struct:  struct   perf_log_record   { \n   int64_t   timestamp ; \n   double   op_latency_ms ; \n   double   cpu_util ; \n   double   mem_avail ; \n   char   log_msg [ 100 ];  };   Note that  log_msg  maps to a  char[100]  rather than an  std::string . To add a new record, we\nwould populate a struct instance, and pass its reference to the append function:  int64_t   ts   =   utils :: time_utils :: cur_ns ();  perf_log_record   rec   =   {   ts ,   2000.0 ,   0.95 ,   0.01 ,   WARN: Server {2, 4, 5} down   };  size_t   off4   =   mlog - append ( rec );   Note that this is a more efficient variant of append, since it avoids the overheads of parsing \nstrings to the corresponding attribute data types.", 
            "title": "Appending Raw bytes"
        }, 
        {
            "location": "/loading_data/#batched-appends", 
            "text": "It is also possible to batch multiple record appends into a single append. The first\nstep in building a batch is to obtain a batch builder:  auto   batch_bldr   =   mlog - get_batch_builder ();   The batch builder supports adding new records via both string vector and raw byte interfaces:  batch_bldr . add_record ({   400 ,   0.85 ,   0.07 ,   WARN: Server {2, 4} down });  perf_log_record   rec   =   {   utils :: time_utils :: cur_ns (),   100.0 ,   0.65 ,   0.25 ,   WARN: Server {2} down   };  batch_bldr . add_record ( rec );   Once the batch is populated, we can append the batch to the Atomic MultiLog as follows:  size_t   off5   =   mlog - append_batch ( batch_bldr . get_batch ());   To understand how we can query the data we have loaded so far, read the guide on  Confluo Queries .", 
            "title": "Batched Appends"
        }, 
        {
            "location": "/loading_data/#stand-alone-mode", 
            "text": "In the stand-alone mode, Confluo runs as a daemon server, serving client requests\nusing Apache Thrift protocol. To start the server, run:  confuod --address = 127 .0.0.1 --port = 9090   Once the server daemon is running, you can send requests to it using the \nC++/Python/Java client APIs. We will focus on the C++ Client API, although \nthe Python/Java API is almost identical. In fact, even the C++ Client API is \nalmost identical to the embedded mode API.  We look at the same performance monitoring and diagnosis tool example for the\nstand-alone mode. The relevant header file to include for the C++ Client API is rpc_client.h .", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/loading_data/#creating-a-client-connection", 
            "text": "To begin with, we first have to establish a client connection with the server.  confluo :: rpc :: rpc_client   client ( 127.0.0.1 ,   9090 );   The first argument to the  rpc_client  constructor corresponds to the server\nhostname, while the second argument corresponds to the server port.", 
            "title": "Creating a Client Connection"
        }, 
        {
            "location": "/loading_data/#creating-a-new-atomic-multilog_1", 
            "text": "We then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); as before, this requires three parameters: a name for the Atomic \nMultiLog, a fixed schema, and a storage mode:  std :: string   schema   =   { \n   timestamp :   LONG , \n   op_latency_ms :   DOUBLE , \n   cpu_util :   DOUBLE , \n   mem_avail :   DOUBLE , \n   log_msg :   STRING ( 100 )  } ;  auto   storage_mode   =   confluo :: storage :: IN_MEMORY ;  client . create_atomic_multilog ( perf_log ,   schema ,   storage_mode );   This operation also internally sets the current Atomic MultiLog \nfor the client to the one we just created (i.e.,  perf_log ). It\nis also possible to explicitly set the current Atomic MultiLog for\nthe client as follows:  client . set_current_atomic_multilog ( perf_log );    Note  It is necessary to set the current Atomic MultiLog for the  rpc_client ;\nissuing requests via the client without setting the current Atomic MultiLog\nwill result in exceptions.", 
            "title": "Creating a New Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#adding-indexes_1", 
            "text": "We can define indexes as follows:  client . add_index ( op_latency_ms );", 
            "title": "Adding Indexes"
        }, 
        {
            "location": "/loading_data/#adding-filters_1", 
            "text": "We can also install filters as follows:  client . add_filter ( low_resources ,   cpu_util 0.8 || mem_avail 0.1 );", 
            "title": "Adding Filters"
        }, 
        {
            "location": "/loading_data/#adding-aggregates_1", 
            "text": "Additionally, we can add aggregates on filters as follows:  client . add_aggregate ( max_latency_ms ,   low_resources ,   MAX(op_latency_ms) );", 
            "title": "Adding Aggregates"
        }, 
        {
            "location": "/loading_data/#installing-triggers_1", 
            "text": "Finally, we can install a trigger on an aggregate as follows:  client . install_trigger ( high_latency_trigger ,   max_latency   1000 );", 
            "title": "Installing Triggers"
        }, 
        {
            "location": "/loading_data/#loading-sample-data-into-atomic-multilog_1", 
            "text": "We are now ready to load some data into the Atomic MultiLog on the server.", 
            "title": "Loading sample data into Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#appending-string-vectors_1", 
            "text": "size_t   off1   =   client . append ({ 100 ,   0.5 ,   0.9 ,    INFO: Launched 1 tasks });  size_t   off2   =   client . append ({ 500 ,   0.9 ,   0.05 ,   WARN: Server {2} down });  size_t   off3   =   client . append ({ 1001 ,   0.9 ,   0.03 ,   WARN: Server {2, 4, 5} down });", 
            "title": "Appending String Vectors"
        }, 
        {
            "location": "/loading_data/#appending-raw-bytes_1", 
            "text": "As with the embedded mode, this version of  append  takes as its input a pointer to a C/C++ struct that maps exactly  to the Atomic MultiLog's schema. Our schema would map to the following C/C++ struct:  struct   perf_log_record   { \n   int64_t   timestamp ; \n   double   op_latency_ms ; \n   double   cpu_util ; \n   double   mem_avail ; \n   char   log_msg [ 100 ];  };   Unlike the embedded interface, to add a new record, we now have to wrap the struct reference in a  record_data  object:  int64_t   ts   =   utils :: time_utils :: cur_ns ();  perf_log_record   rec   =   {   ts ,   2000.0 ,   0.95 ,   0.01 ,   WARN: Server {2, 4, 5} down   };  size_t   off4   =   client . append ( confluo :: rpc :: record_data ( rec ,   sizeof ( rec )));   As before, this is a more efficient variant of append, since it avoids the overheads of parsing \nstrings to the corresponding attribute data types.", 
            "title": "Appending Raw bytes"
        }, 
        {
            "location": "/loading_data/#batched-appends_1", 
            "text": "It is also possible to batch multiple record appends into a single append operation via the client API. \nThis is particularly useful since batching helps amortize the cost of network latency.  The first step in building a batch is to obtain a batch builder:  auto   batch_bldr   =   client . get_batch_builder ();   The batch builder supports adding new records via both string vector and raw byte interfaces:  batch_bldr . add_record ({   400 ,   0.85 ,   0.07 ,   WARN: Server {2, 4} down });  perf_log_record   rec   =   {   utils :: time_utils :: cur_ns (),   100.0 ,   0.65 ,   0.25 ,   WARN: Server {2} down   };  batch_bldr . add_record ( confluo :: rpc :: record_data ( rec ,   sizeof ( rec )));   Once the batch is populated, we can append the batch as follows:  size_t   off5   =   client . append_batch ( batch_bldr . get_batch ());   Details on querying the data via the client interface can be found in the guide on  Confluo Queries .      We plan on adding support for variable width data types in a future relase.", 
            "title": "Batched Appends"
        }, 
        {
            "location": "/type_system/", 
            "text": "Confluo Type System\n\n\nConfluo uses a strictly typed system. While primitive data types like\n\nBOOL\n, \nCHAR\n, \nSHORT\n, \nINT\n, \nLONG\n, \nFLOAT\n, \nDOUBLE\n and \nSTRING\n\nare supported by default in Confluo, it is possible to add custom \nuser-defined data types. This requires defining a few operations that would\nallow operations like applying filters and triggers on attributes of\nthe custom data type.\n\n\nTo create a new type, we need to define the following properties so that\nnative operations can be supported; these properties are summarised in the\n\ntype_properties\n \nstruct:\n\n\n\n\nstd::string name\n - A unique name for the type\n\n\nsize_t size\n -  The size of underlying representation for fixed sized types. This should be set to zero for \ndynamically sized types (e.g., see definintion for \nSTRING\n type).\n\n\nvoid* min\n - This is a pointer to the minimum value that the type\ncan hold. See \ntype_properties.h\n \nto see examples of \nmin\n assigned to primitive types.\n\n\nvoid* max\n - This is a pointer to the maximum value that the type\ncan hold. See \ntype_properties.h\n \nto see examples of \nmax\n assigned to primitive types.\n\n\nvoid* one\n - This is a pointer to the step value with which the type\ncan be incremented. See \ntype_properties.h\n \nto see examples of \none\n assigned to primitive types.\n\n\nvoid* zero\n - This is a pointer to the zero value for the type. See \n\ntype_properties.h\n to see \nexamples of \nzero\n assigned to primitive types.\n\n\nbool is_numeric\n - This indicates whether the type is numeric or not; \nnumeric types typically support most arithmetic operators; see \n[arithmetic_ops.h][../libconfluo/confluo/types/arithmetic_ops.h] for examples.\n\n\nrelational_ops_t relational_ops\n - Stores a list of relational operator functions\nfor the given type, so that operations like \nfilter\n can work. See \n\nrel_ops.h\n for examples of\nwhat relational functions can be defined.\n\n\nbinary_ops_t binary_ops\n - Stores a list of binary arithmetic operator functions\nfor the given type, so that operations like filter can accurately be applied\nto the type. Check \narithmetic_ops.h\n\nfor examples of binary operator functions that can be defined.\n\n\nunary_ops_t unary_ops\n - Stores a list of unary arithmetic operator functions\nfor the given type, so that operations like filter can work for the given\ntype. Check \narithmetic_ops.h\n for\nexamples of unary function operators that can be defined.\n\n\nkey_op_t key_transform_op\n - Stores the key-transform function. This function \nis important for looking up attributes of the type in an index; see \n\nkey_ops.h\n for example\ndefinitions of key_transform.\n\n\nparse_op_t parse_op\n - Parses data instance from a string representation of this type. See \n\nstring_ops.h\n for examples.\n\n\nto_string_op_t to_stirng_op\n - Converts data instance of the type to its string representation. See \n\nstring_ops.h\n for examples.\n\n\nserialize_op_t serialize_op\n - Serializes the underlying data representation of the type into raw bytes;\nsee \nserde_ops.h\n for examples.\n\n\ndeserialize_op_t deserialize_op\n - Reads the raw byte representation of the type and parses it to data;\nsee \nserde_ops.h\n for examples.\n\n\n\n\nExample declarations of user-defined types can be found at\n\nip_address.h\n and \n\nsize_type.h\n.\n\n\nOnce the properties for custom type is defined in the \ntype_properties\n struct, \nit needs to be registered with Confluo's \ntype manager\n \nvia the \ntype_manager::register_type\n interface. Once registered, a useful symbolic\nreference to the data type, wrapped in a \ndata_type\n object, can be obtained via\nthe \ntype_manager::get_type\n interface.\n\n\nWith this object, it is possible to add new columns of this type in any \nAtomic MultiLog. From here on out, appending records to the\nAtomic MultiLog, along with operations like filters and triggers, will work out\nof the box.\n\n\nSee \ntype_manager_test.h\n for\nexamples of how to build a schema and add records with user-defined\ntypes.", 
            "title": "Type System"
        }, 
        {
            "location": "/type_system/#confluo-type-system", 
            "text": "Confluo uses a strictly typed system. While primitive data types like BOOL ,  CHAR ,  SHORT ,  INT ,  LONG ,  FLOAT ,  DOUBLE  and  STRING \nare supported by default in Confluo, it is possible to add custom \nuser-defined data types. This requires defining a few operations that would\nallow operations like applying filters and triggers on attributes of\nthe custom data type.  To create a new type, we need to define the following properties so that\nnative operations can be supported; these properties are summarised in the type_properties  \nstruct:   std::string name  - A unique name for the type  size_t size  -  The size of underlying representation for fixed sized types. This should be set to zero for \ndynamically sized types (e.g., see definintion for  STRING  type).  void* min  - This is a pointer to the minimum value that the type\ncan hold. See  type_properties.h  \nto see examples of  min  assigned to primitive types.  void* max  - This is a pointer to the maximum value that the type\ncan hold. See  type_properties.h  \nto see examples of  max  assigned to primitive types.  void* one  - This is a pointer to the step value with which the type\ncan be incremented. See  type_properties.h  \nto see examples of  one  assigned to primitive types.  void* zero  - This is a pointer to the zero value for the type. See  type_properties.h  to see \nexamples of  zero  assigned to primitive types.  bool is_numeric  - This indicates whether the type is numeric or not; \nnumeric types typically support most arithmetic operators; see \n[arithmetic_ops.h][../libconfluo/confluo/types/arithmetic_ops.h] for examples.  relational_ops_t relational_ops  - Stores a list of relational operator functions\nfor the given type, so that operations like  filter  can work. See  rel_ops.h  for examples of\nwhat relational functions can be defined.  binary_ops_t binary_ops  - Stores a list of binary arithmetic operator functions\nfor the given type, so that operations like filter can accurately be applied\nto the type. Check  arithmetic_ops.h \nfor examples of binary operator functions that can be defined.  unary_ops_t unary_ops  - Stores a list of unary arithmetic operator functions\nfor the given type, so that operations like filter can work for the given\ntype. Check  arithmetic_ops.h  for\nexamples of unary function operators that can be defined.  key_op_t key_transform_op  - Stores the key-transform function. This function \nis important for looking up attributes of the type in an index; see  key_ops.h  for example\ndefinitions of key_transform.  parse_op_t parse_op  - Parses data instance from a string representation of this type. See  string_ops.h  for examples.  to_string_op_t to_stirng_op  - Converts data instance of the type to its string representation. See  string_ops.h  for examples.  serialize_op_t serialize_op  - Serializes the underlying data representation of the type into raw bytes;\nsee  serde_ops.h  for examples.  deserialize_op_t deserialize_op  - Reads the raw byte representation of the type and parses it to data;\nsee  serde_ops.h  for examples.   Example declarations of user-defined types can be found at ip_address.h  and  size_type.h .  Once the properties for custom type is defined in the  type_properties  struct, \nit needs to be registered with Confluo's  type manager  \nvia the  type_manager::register_type  interface. Once registered, a useful symbolic\nreference to the data type, wrapped in a  data_type  object, can be obtained via\nthe  type_manager::get_type  interface.  With this object, it is possible to add new columns of this type in any \nAtomic MultiLog. From here on out, appending records to the\nAtomic MultiLog, along with operations like filters and triggers, will work out\nof the box.  See  type_manager_test.h  for\nexamples of how to build a schema and add records with user-defined\ntypes.", 
            "title": "Confluo Type System"
        }, 
        {
            "location": "/queries/", 
            "text": "Querying Data\n\n\nQueries in Confluo can either be \nonline\n or \noffline\n. Online queries are executed as new data records\nare written to an Atomic MultiLog, while offline queries are evaluated on already written records.\nIn essence, online queries are similar to continuous queries databases.\n\n\nIn order to support online and offline queries, Confluo makes use of indexes, filters, aggregates \nand triggers. The interface for adding these elements to an Atomic MultiLog was described in the \nguide on \nData Storage and Loading\n.\n\n\nTo see how Confluo supports online and offline queries, see the individual guides at:\n\n\n\n\nOnline Queries\n\n\nOffline Queries", 
            "title": "Queries Overview"
        }, 
        {
            "location": "/queries/#querying-data", 
            "text": "Queries in Confluo can either be  online  or  offline . Online queries are executed as new data records\nare written to an Atomic MultiLog, while offline queries are evaluated on already written records.\nIn essence, online queries are similar to continuous queries databases.  In order to support online and offline queries, Confluo makes use of indexes, filters, aggregates \nand triggers. The interface for adding these elements to an Atomic MultiLog was described in the \nguide on  Data Storage and Loading .  To see how Confluo supports online and offline queries, see the individual guides at:   Online Queries  Offline Queries", 
            "title": "Querying Data"
        }, 
        {
            "location": "/online_queries/", 
            "text": "Online Queries\n\n\nOnline queries on Confluo are executed automatically as new records are written \ninto an Atomic MultiLog. Therefore, these queries need to be pre-defined; the \nguide on \nData Storage\n describes how we can define filters,\naggregates and triggers for online evaluation. In this guide, we will look\nat how we can retrieve the results of these online queries.\n\n\nEmbedded Mode\n\n\nWe begin with the embedded mode of operation. We work with the assumption that\nthe Atomic MultiLog has already been created, and filters, aggregates and triggers\nhave been added to it as outlined in \nData Storage\n.\n\n\nTo start with, we have a reference to the Atomic MultiLog as follows:\n\n\nconfluo\n::\natomic_multilog\n*\n \nmlog\n \n=\n \nstore\n.\nget_atomic_multilog\n(\nperf_log\n);\n\n\n\n\n\n\nQuerying Pre-defined Filters\n\n\nWe can query a pre-defined filter as follows:\n\n\nauto\n \nrecord_stream\n \n=\n \nmlog\n-\nquery_filter\n(\nlow_resources\n,\n \n0\n,\n \nUINT64_MAX\n);\n\n\nfor\n \n(\nauto\n \ns\n \n=\n \nrecord_stream\n;\n \n!\ns\n.\nempty\n();\n \ns\n \n=\n \ns\n.\ntail\n())\n \n{\n\n  \nstd\n::\ncout\n \n \ns\n.\nhead\n().\nto_string\n();\n\n\n}\n\n\n\n\n\n\nThe first parameter corresponds to the name of the filter to be queried, while \nthe second and third parameters correspond to the begining timestamp and end \ntimestamp to consider for records in the filter. We've specified them to capture\nall possible values of timestamp. This operation returns a lazily evaluated record \nstream which supports functional semantics such as filter, map, etc. See Confluo's\n\nStream API\n \nfor more details on how to work with lazy streams.\n\n\nObtaining Pre-defined Aggregates\n\n\nWe can obtian a pre-defined aggregate as follows:\n\n\nauto\n \nvalue\n \n=\n \nmlog\n-\nget_aggregate\n(\nmax_latency_ms\n,\n \n0\n,\n \nUINT64_MAX\n);\n\n\nstd\n::\ncout\n \n \nvalue\n.\nto_string\n();\n\n\n\n\n\n\nThe operation takes the name of the aggregate as its first parameter, while the \nsecond and third parameters correspond to begin and end timestmaps, as with \npre-defined filters. The query returns a \n\nnumeric\n \nobject, which is a wrapper around numeric values in C++.\n\n\nObtaining Alerts from a Pre-defined Trigger\n\n\nFinally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:\n\n\nauto\n \nalert_stream\n \n=\n \nmlog\n-\nget_alerts\n(\n0\n,\n \nUINT64_MAX\n,\n \nhigh_latency_trigger\n);\n\n\nfor\n \n(\nauto\n \ns\n \n=\n \nalert_stream\n;\n \n!\ns\n.\nempty\n();\n \ns\n \n=\n \ns\n.\ntail\n())\n \n{\n\n  \nstd\n::\ncout\n \n \ns\n.\nhead\n().\nto_string\n();\n\n\n}\n\n\n\n\n\n\nThe query takes begin and end timestamps as its first and second arguments,\nand an optional trigger name as its third argument. The query returns a lazy \nstream over generated alerts for this trigger in the specified time-range.\n\n\nStand-alone Mode\n\n\nThe API for Stand-alone mode of operation is quite similar to the embedded mode.\nWe only focus on the C++ Client API, since Python and Java Client APIs are\nalmost identical to the C++ Client API.\n\n\nAs with the embedded mode, we work with the assumption that the client is connected to the server, \nhas already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers.\nAlso, the current Atomic MultiLog for the client has been set to \nperf_log\n as follows:\n\n\nclient\n.\nset_current_atomic_multilog\n(\nperf_log\n);\n\n\n\n\n\n\nQuerying Pre-defined Filters\n\n\nWe can query a pre-defined filter as follows:\n\n\nauto\n \nrecord_stream\n \n=\n \nclient\n.\nquery_filter\n(\nlow_resources\n,\n \n0\n,\n \nUINT64_MAX\n);\n\n\nfor\n \n(\nauto\n \ns\n \n=\n \nrecord_stream\n;\n \n!\ns\n.\nempty\n();\n \n++\ns\n)\n \n{\n\n  \nstd\n::\ncout\n \n \ns\n.\nget\n().\nto_string\n();\n\n\n}\n\n\n\n\n\n\nThis operation returns a lazy stream of records, which automatically fetches\nmore data from the server as the clients consumes them.\n\n\nObtaining Pre-defined Aggregates\n\n\nWe can obtian a pre-defined aggregate as follows:\n\n\nstd\n::\nstring\n \nvalue\n \n=\n \nclient\n.\nget_aggregate\n(\nmax_latency_ms\n,\n \n0\n,\n \nUINT64_MAX\n);\n\n\nstd\n::\ncout\n \n \nvalue\n;\n\n\n\n\n\n\nThe operation returns a string representation of the aggregate.\n\n\nObtaining Alerts from a Pre-defined Trigger\n\n\nFinally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:\n\n\nauto\n \nalert_stream\n \n=\n \nclient\n.\nget_alerts\n(\n0\n,\n \nUINT64_MAX\n,\n \nhigh_latency_trigger\n);\n\n\nfor\n \n(\nauto\n \ns\n \n=\n \nalert_stream\n;\n \n!\ns\n.\nempty\n();\n \n++\ns\n)\n \n{\n\n  \nstd\n::\ncout\n \n \ns\n.\nget\n();\n\n\n}\n\n\n\n\n\n\nSimilar to the filter query, this operation returns a lazy stream of alerts, \nwhich automatically fetches more data from the server as the clients consumes them.", 
            "title": "Online Queries"
        }, 
        {
            "location": "/online_queries/#online-queries", 
            "text": "Online queries on Confluo are executed automatically as new records are written \ninto an Atomic MultiLog. Therefore, these queries need to be pre-defined; the \nguide on  Data Storage  describes how we can define filters,\naggregates and triggers for online evaluation. In this guide, we will look\nat how we can retrieve the results of these online queries.", 
            "title": "Online Queries"
        }, 
        {
            "location": "/online_queries/#embedded-mode", 
            "text": "We begin with the embedded mode of operation. We work with the assumption that\nthe Atomic MultiLog has already been created, and filters, aggregates and triggers\nhave been added to it as outlined in  Data Storage .  To start with, we have a reference to the Atomic MultiLog as follows:  confluo :: atomic_multilog *   mlog   =   store . get_atomic_multilog ( perf_log );", 
            "title": "Embedded Mode"
        }, 
        {
            "location": "/online_queries/#querying-pre-defined-filters", 
            "text": "We can query a pre-defined filter as follows:  auto   record_stream   =   mlog - query_filter ( low_resources ,   0 ,   UINT64_MAX );  for   ( auto   s   =   record_stream ;   ! s . empty ();   s   =   s . tail ())   { \n   std :: cout     s . head (). to_string ();  }   The first parameter corresponds to the name of the filter to be queried, while \nthe second and third parameters correspond to the begining timestamp and end \ntimestamp to consider for records in the filter. We've specified them to capture\nall possible values of timestamp. This operation returns a lazily evaluated record \nstream which supports functional semantics such as filter, map, etc. See Confluo's Stream API  \nfor more details on how to work with lazy streams.", 
            "title": "Querying Pre-defined Filters"
        }, 
        {
            "location": "/online_queries/#obtaining-pre-defined-aggregates", 
            "text": "We can obtian a pre-defined aggregate as follows:  auto   value   =   mlog - get_aggregate ( max_latency_ms ,   0 ,   UINT64_MAX );  std :: cout     value . to_string ();   The operation takes the name of the aggregate as its first parameter, while the \nsecond and third parameters correspond to begin and end timestmaps, as with \npre-defined filters. The query returns a  numeric  \nobject, which is a wrapper around numeric values in C++.", 
            "title": "Obtaining Pre-defined Aggregates"
        }, 
        {
            "location": "/online_queries/#obtaining-alerts-from-a-pre-defined-trigger", 
            "text": "Finally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:  auto   alert_stream   =   mlog - get_alerts ( 0 ,   UINT64_MAX ,   high_latency_trigger );  for   ( auto   s   =   alert_stream ;   ! s . empty ();   s   =   s . tail ())   { \n   std :: cout     s . head (). to_string ();  }   The query takes begin and end timestamps as its first and second arguments,\nand an optional trigger name as its third argument. The query returns a lazy \nstream over generated alerts for this trigger in the specified time-range.", 
            "title": "Obtaining Alerts from a Pre-defined Trigger"
        }, 
        {
            "location": "/online_queries/#stand-alone-mode", 
            "text": "The API for Stand-alone mode of operation is quite similar to the embedded mode.\nWe only focus on the C++ Client API, since Python and Java Client APIs are\nalmost identical to the C++ Client API.  As with the embedded mode, we work with the assumption that the client is connected to the server, \nhas already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers.\nAlso, the current Atomic MultiLog for the client has been set to  perf_log  as follows:  client . set_current_atomic_multilog ( perf_log );", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/online_queries/#querying-pre-defined-filters_1", 
            "text": "We can query a pre-defined filter as follows:  auto   record_stream   =   client . query_filter ( low_resources ,   0 ,   UINT64_MAX );  for   ( auto   s   =   record_stream ;   ! s . empty ();   ++ s )   { \n   std :: cout     s . get (). to_string ();  }   This operation returns a lazy stream of records, which automatically fetches\nmore data from the server as the clients consumes them.", 
            "title": "Querying Pre-defined Filters"
        }, 
        {
            "location": "/online_queries/#obtaining-pre-defined-aggregates_1", 
            "text": "We can obtian a pre-defined aggregate as follows:  std :: string   value   =   client . get_aggregate ( max_latency_ms ,   0 ,   UINT64_MAX );  std :: cout     value ;   The operation returns a string representation of the aggregate.", 
            "title": "Obtaining Pre-defined Aggregates"
        }, 
        {
            "location": "/online_queries/#obtaining-alerts-from-a-pre-defined-trigger_1", 
            "text": "Finally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:  auto   alert_stream   =   client . get_alerts ( 0 ,   UINT64_MAX ,   high_latency_trigger );  for   ( auto   s   =   alert_stream ;   ! s . empty ();   ++ s )   { \n   std :: cout     s . get ();  }   Similar to the filter query, this operation returns a lazy stream of alerts, \nwhich automatically fetches more data from the server as the clients consumes them.", 
            "title": "Obtaining Alerts from a Pre-defined Trigger"
        }, 
        {
            "location": "/offline_queries/", 
            "text": "Offline Queries\n\n\nEmbedded Mode\n\n\nNow we take a look at how we can query the data in the Atomic MultiLog. First,\nit is straightforward to retrieve records given their offsets:\n\n\nauto\n \nrecord1\n \n=\n \nmlog\n-\nread\n(\noff1\n);\n\n\nauto\n \nrecord2\n \n=\n \nmlog\n-\nread\n(\noff2\n);\n\n\nauto\n \nrecord3\n \n=\n \nmlog\n-\nread\n(\noff3\n);\n\n\n\n\n\n\nEach of \nrecord1\n, \nrecord2\n, and \nrecord3\n are vectors of strings.\n\n\nWe can query indexed attributes as follows:\n\n\nauto\n \nrecord_stream1\n \n=\n \nmlog\n-\nexecute_filter\n(\ntype == b\n);\n\n\nfor\n \n(\nauto\n \ns\n \n=\n \nrecord_stream1\n;\n \n!\ns\n.\nempty\n();\n \ns\n \n=\n \ns\n.\ntail\n())\n \n{\n\n  \nstd\n::\ncout\n \n \ns\n.\nhead\n().\nto_string\n();\n\n\n}\n\n\n\n\n\n\nNote that the operation returns a lazily evaluated stream, which supports\nfunctional style operations like map, filter, etc. See \n\nStream API\n\nfor more details.\n\n\nStand-alone Mode", 
            "title": "Offline Queries"
        }, 
        {
            "location": "/offline_queries/#offline-queries", 
            "text": "", 
            "title": "Offline Queries"
        }, 
        {
            "location": "/offline_queries/#embedded-mode", 
            "text": "Now we take a look at how we can query the data in the Atomic MultiLog. First,\nit is straightforward to retrieve records given their offsets:  auto   record1   =   mlog - read ( off1 );  auto   record2   =   mlog - read ( off2 );  auto   record3   =   mlog - read ( off3 );   Each of  record1 ,  record2 , and  record3  are vectors of strings.  We can query indexed attributes as follows:  auto   record_stream1   =   mlog - execute_filter ( type == b );  for   ( auto   s   =   record_stream1 ;   ! s . empty ();   s   =   s . tail ())   { \n   std :: cout     s . head (). to_string ();  }   Note that the operation returns a lazily evaluated stream, which supports\nfunctional style operations like map, filter, etc. See  Stream API \nfor more details.", 
            "title": "Embedded Mode"
        }, 
        {
            "location": "/offline_queries/#stand-alone-mode", 
            "text": "", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/data_archival/", 
            "text": "Archiving Data\n\n\n\n\nNote\n\n\nThis is work in progress.\n\n\n\n\nDocumentation for Data Archival will be added soon.", 
            "title": "Archiving Data"
        }, 
        {
            "location": "/data_archival/#archiving-data", 
            "text": "Note  This is work in progress.   Documentation for Data Archival will be added soon.", 
            "title": "Archiving Data"
        }, 
        {
            "location": "/cpp_api/", 
            "text": "C++ API Documentation\n\n\nThis page will be replaced by API documentation generated by Doxygen.", 
            "title": "C++"
        }, 
        {
            "location": "/cpp_api/#c-api-documentation", 
            "text": "This page will be replaced by API documentation generated by Doxygen.", 
            "title": "C++ API Documentation"
        }, 
        {
            "location": "/client_api/", 
            "text": "Client API", 
            "title": "Overview"
        }, 
        {
            "location": "/client_api/#client-api", 
            "text": "", 
            "title": "Client API"
        }, 
        {
            "location": "/cpp_client_api/", 
            "text": "C++ Client API Documentation\n\n\nThis page will be replaced by API documentation generated by Doxygen.", 
            "title": "C++"
        }, 
        {
            "location": "/cpp_client_api/#c-client-api-documentation", 
            "text": "This page will be replaced by API documentation generated by Doxygen.", 
            "title": "C++ Client API Documentation"
        }, 
        {
            "location": "/python_client_api/", 
            "text": "Python Client API Documentation\n\n\nThis page will be replaced by API documentation generated by Sphinx.", 
            "title": "Python"
        }, 
        {
            "location": "/python_client_api/#python-client-api-documentation", 
            "text": "This page will be replaced by API documentation generated by Sphinx.", 
            "title": "Python Client API Documentation"
        }, 
        {
            "location": "/java_client_api/", 
            "text": "Java Client API Documentation\n\n\nThis page will be replaced by API documentation generated by javadocs.", 
            "title": "Java"
        }, 
        {
            "location": "/java_client_api/#java-client-api-documentation", 
            "text": "This page will be replaced by API documentation generated by javadocs.", 
            "title": "Java Client API Documentation"
        }
    ]
}