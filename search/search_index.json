{
    "docs": [
        {
            "location": "/", 
            "text": "Confluo\n\n\nOverview\n\n\nConfluo is a system for real-time monitoring and analysis of data, that supports:\n\n\n\n\nhigh-throughput concurrent writes of millions of data points from multiple data streams;\n\n\nonline queries at millisecond timescale; and \n\n\nad-hoc queries using minimal CPU resources.\n\n\n\n\nUser Guide\n\n\n\n\nQuick Start\n\n\nInstallation\n\n\nModes of Operation\n\n\nData Storage\n\n\nType System\n\n\n\n\n\n\nQuerying Data\n\n\nOnline Queries\n\n\nOffline Queries\n\n\n\n\n\n\nArchiving Data\n\n\n\n\nAPI Docs\n\n\n\n\nC++ API\n\n\nClient APIs\n\n\nC++ Client API\n\n\nPython Client API\n\n\nJava Client API", 
            "title": "About"
        }, 
        {
            "location": "/#confluo", 
            "text": "", 
            "title": "Confluo"
        }, 
        {
            "location": "/#overview", 
            "text": "Confluo is a system for real-time monitoring and analysis of data, that supports:   high-throughput concurrent writes of millions of data points from multiple data streams;  online queries at millisecond timescale; and   ad-hoc queries using minimal CPU resources.", 
            "title": "Overview"
        }, 
        {
            "location": "/#user-guide", 
            "text": "Quick Start  Installation  Modes of Operation  Data Storage  Type System    Querying Data  Online Queries  Offline Queries    Archiving Data", 
            "title": "User Guide"
        }, 
        {
            "location": "/#api-docs", 
            "text": "C++ API  Client APIs  C++ Client API  Python Client API  Java Client API", 
            "title": "API Docs"
        }, 
        {
            "location": "/research/", 
            "text": "Research Papers\n\n\nConfluo: Distributed Monitoring and Diagnosis Stack for High-speed Networks\n \n\nAnurag Khandelwal, Rachit Agarwal, Ion Stoica \n\nNSDI, Boston, MA, Feb 2019", 
            "title": "Research Papers"
        }, 
        {
            "location": "/research/#research-papers", 
            "text": "Confluo: Distributed Monitoring and Diagnosis Stack for High-speed Networks   \nAnurag Khandelwal, Rachit Agarwal, Ion Stoica  \nNSDI, Boston, MA, Feb 2019", 
            "title": "Research Papers"
        }, 
        {
            "location": "/contributing/", 
            "text": "Contributing", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#contributing", 
            "text": "", 
            "title": "Contributing"
        }, 
        {
            "location": "/contact/", 
            "text": "Contact\n\n\nThe best way to discuss Confluo feature requests and bug reportss is via \n\nGitHub issues\n.\n\n\nPeople\n\n\nAnurag Khandelwal\n \n\nGraduate Student at UC Berkeley\n\nanuragk AT berkeley.edu\n\n\nRachit Agarwal\n \n\nAssistant Professor at Cornell University\n\nragarwal AT cs.cornell.edu\n\n\nIon Stoica\n \n\nProfessor at UC Berkeley\n\nistoica AT berkeley.edu\n\n\nConfluo also benefits from contributions from amazing current and past \nundergraduate students at UC Berkeley: \n\n\n\n\nUjval Misra\n\n\nNeil Giridharan\n\n\nSritej Attaluri", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#contact", 
            "text": "The best way to discuss Confluo feature requests and bug reportss is via  GitHub issues .", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#people", 
            "text": "Anurag Khandelwal   \nGraduate Student at UC Berkeley \nanuragk AT berkeley.edu  Rachit Agarwal   \nAssistant Professor at Cornell University \nragarwal AT cs.cornell.edu  Ion Stoica   \nProfessor at UC Berkeley \nistoica AT berkeley.edu  Confluo also benefits from contributions from amazing current and past \nundergraduate students at UC Berkeley:    Ujval Misra  Neil Giridharan  Sritej Attaluri", 
            "title": "People"
        }, 
        {
            "location": "/quick_start/", 
            "text": "Quick Start\n\n\nIn this Quick Start, we will take a look at how to download and setup Confluo,\nload some sample data, and query it.\n\n\nPre-requisites\n\n\n\n\nMacOS X or Unix-based OS; Windows is not yet supported.\n\n\nC++ compiler that supports C++11 standard\n\n\nCMake 2.8 or later\n\n\nBoost 1.53 or later\n\n\n\n\nFor python client, you will additionally require:\n\n\n\n\nPython 2.7 or later\n\n\nPython Packages: six 1.7.2 or later\n\n\n\n\nFor java client, you will additionally require:\n\n\n\n\nJava 1.7 or later\n\n\nant 1.6.2 or later\n\n\n\n\nDownload and Install\n\n\nTo download and install Confluo, use the following commands:\n\n\ngit clone https://github.com/ucbrise/confluo.git\ncd confluo\nmkdir build\ncd build\ncmake ..\nmake -j \n make test \n make install\n\n\n\n\nUsing Confluo\n\n\nConfluo can be used in two modes -- embedded and stand-alone. In the embedded mode,\nConfluo is used as a header-only library in C++, allowing Confluo to use the same\naddress-space as the application process. In the stand-alone mode, Confluo runs\nas a daemon server process, allowing clients to communicate with it using \n\nApache Thrift\n protocol.\n\n\nEmbedded Mode\n\n\nIn order to use Confluo in the embedded mode, we simply need to include\nConfluo's header files under libconfluo/confluo, use the Confluo C++ API in \na C++ application, and compile using a modern C++ compiler. The entry point \nheader file to include is \nconfluo_store.h\n. \n\n\nWe will first create a new Confluo Store with the data path for it to use \nas follows:\n\n\nconfluo::confluo_store store(\n/path/to/data\n);\n\n\n\n\nWe then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); this requires three parameters: name, schema, and the storage mode:\n\n\nstd::string schema = \n{\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n}\n;\nauto storage_mode = confluo::storage::IN_MEMORY;\nstore.create_atomic_multilog(\nperf_log\n, schema, storage_mode);\n\n\n\n\nOur schema contains 5 attributes: a signed 8-byte integer timestamp,\ndouble floating-point precision operation latency (in ms), CPU utilization\nand available memory, and a string log message field (upper bounded to 100 \ncharacters). \n\n\nWe then obtain a reference to our newly created Atomic MultiLog:\n\n\nconfluo::atomic_multilog* mlog = store.get_atomic_multilog(\nperf_log\n);\n\n\n\n\nWe can define indexes on the Atomic MultiLog as follows:\n\n\nmlog-\nadd_index(\nop_latency_ms\n);\n\n\n\n\nto add an index on \nop_latency_ms\n attribute. We can also install filters as follows:\n\n\nmlog-\nadd_filter(\nlow_resources\n, \ncpu_util\n0.8 || mem_avail\n0.1\n);\n\n\n\n\nto explicitly filter out records that indicate low system resources (CPU \nutilization \n 80%, Available Memory \n 10%), using a filter named \nlow_resources\n.\n\n\nAdditionally, we can add aggregates on filters as follows:\n\n\nmlog-\nadd_aggregate(\nmax_latency_ms\n, \nlow_resources\n, \nMAX(op_latency_ms)\n);\n\n\n\n\nThis adds a new stored aggregate \nmax_latency_ms\n on the filter \n\nlow_resources\n we defined before. In essence, it records the highest \noperation latency reported in any record that also indicated low \navailable resources.\n\n\nFinally, we can install a trigger on aggregates as follows:\n\n\nmlog-\ninstall_trigger(\nhigh_latency_trigger\n, \nmax_latency \n 1000\n);\n\n\n\n\nThis installs a trigger \nhigh_latency_trigger\n on the aggregate \n\nmax_latency_ms\n, which should generate an alert whenever the \ncondition \nmax_latency_ms \n 1000\n is satisfied, i.e.,\nwhenever the maximum latency for an operation exceeds 1s and\nthe available resources are low.\n\n\nWe are now ready to load some data into this multilog:\n\n\nsize_t off1 = mlog-\nappend({\n100\n, \n0.5\n, \n0.9\n,  \nINFO: Launched 1 tasks\n});\nsize_t off2 = mlog-\nappend({\n500\n, \n0.9\n, \n0.05\n, \nWARN: Server {2} down\n});\nsize_t off3 = mlog-\nappend({\n1001\n, \n0.9\n, \n0.03\n, \nWARN: Server {2, 4, 5} down\n});\n\n\n\n\nNote that the \nappend\n method takes a vector of strings as its input, where\nthe vector corresponds to a single record. The number of entries in the\nvector must match the number of entries in the schema, with the exception\nof the timestamp --- if the timestamp is not provided, Confluo will automatically\nassign one.\n\n\nAlso note that the operation returns a unique offset corresponding to each append\noperation. This forms the \"key\" for records stored in the Atomic MultiLog -- records\ncan be retrieved by specifying their corresponding offsets.\n\n\nNow we take a look at how we can query the data in the Atomic MultiLog. First,\nit is straightforward to retrieve records given their offsets:\n\n\nauto record1 = mlog-\nread(off1);\nauto record2 = mlog-\nread(off2);\nauto record3 = mlog-\nread(off3);\n\n\n\n\nEach of \nrecord1\n, \nrecord2\n, and \nrecord3\n are vectors of strings.\n\n\nWe can query indexed attributes as follows:\n\n\nauto record_stream1 = mlog-\nexecute_filter(\ncpu_util\n0.5 || mem_avail\n0.5\n);\nfor (auto s = record_stream1; !s.empty(); s = s.tail()) {\n  std::cout \n s.head().to_string();\n}\n\n\n\n\nNote that the operation returns a lazily evaluated stream, which supports\nfunctional style operations like map, filter, etc. See \n\nstream.h\n\nfor more details.\n\n\nWe can also query the defined filter as follows:\n\n\nauto record_stream2 = mlog-\nquery_filter(\nlow_resources\n, 0, UINT64_MAX);\nfor (auto s = record_stream2; !s.empty(); s = s.tail()) {\n  std::cout \n s.head().to_string();\n}\n\n\n\n\nThe first parameter corresponds to the name of the filter to be queried, while \nthe second and third parameters correspond to the begining timestamp and end \ntimestamp to consider for records in the filter. We've specified them to capture\nall possible values of timestamp. Similar to the \nexecute_filter\n query, this\noperation also returns a lazily evaluated record stream.\n\n\nWe query aggregates as follows:\n\n\nauto value = mlog-\nget_aggregate(\nmax_latency_ms\n, 0, UINT64_MAX);\nstd::cout \n value.to_string();\n\n\n\n\nThe query takes the name of the aggregate as its first parameter, while the \nsecond and third parameters correspond to begin and end timestmaps, as before.\nThe query returns a \nnumeric\n \nobject, which is a wrapper around numeric values.\n\n\nFinally, we can query the generated alerts by triggers we have installed as\nfollows:\n\n\nauto alert_stream = mlog-\nget_alerts(0, UINT64_MAX, \nhigh_latency_trigger\n);\nfor (auto s = alert_stream; !s.empty(); s = s.tail()) {\n  std::cout \n s.head().to_string();\n}\n\n\n\n\nThe query takes and begin and end timestamps as its first and second arguments,\nand an optional trigger name as its third argument. The query returns a lazy \nstream over generated alerts for this trigger in the specified time-range.\n\n\nSee API docs for \nC++\n and in-depth user guides on \nData Storage\n\nand \nConflo Queries\n for details on Confluo's supported operations.\n\n\nStand-alone Mode\n\n\nIn the stand-alone mode, Confluo runs as a daemon server, serving client requests\nusing Apache Thrift protocol. To start the server, run:\n\n\nconfuod --address=127.0.0.1 --port=9090\n\n\n\n\nOnce the server daemon is running, you can query it using the C++, Python or Java \nclient APIs. The client APIs closely resemble the embedded API. In this guide, \nwe will focus on the C++ client API, although client APIs for Python and Java are\nalmost identical.\n\n\nWe first create a new client connection to the Confluo daemon:\n\n\nconfluo::rpc::rpc_client client(\n127.0.0.1\n, 9090);\n\n\n\n\nThe first argument to the \nrpc_client\n constructor corresponds to the server\nhostname, while the second argument corresponds to the server port.\n\n\nWe then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); as before, this requires three parameters: a name for the Atomic \nMultiLog, a fixed schema, and a storage mode:\n\n\nstd::string schema = \n{\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n}\n;\nauto storage_mode = confluo::storage::IN_MEMORY;\nclient.create_atomic_multilog(\nperf_log\n, schema, storage_mode);\n\n\n\n\nThis operation also internally sets the current Atomic MultiLog \nfor the client to the one we just created (i.e., \nperf_log\n).\n\n\nWe can define indexes as follows:\n\n\nclient.add_index(\nop_latency_ms\n);\n\n\n\n\nWe can also install filters as follows:\n\n\nclient.add_filter(\nlow_resources\n, \ncpu_util\n0.8 || mem_avail\n0.1\n);\n\n\n\n\nAdditionally, we can add aggregates on filters as follows:\n\n\nclient.add_aggregate(\nmax_latency_ms\n, \nlow_resources\n, \nMAX(op_latency_ms)\n);\n\n\n\n\nFinally, we can install a trigger on an aggregate as follows:\n\n\nclient.install_trigger(\nhigh_latency_trigger\n, \nmax_latency \n 1000\n);\n\n\n\n\nTo load data into the Atomic MultiLog:\n\n\nsize_t off1 = client.append({\n100\n, \n0.5\n, \n0.9\n,  \nINFO: Launched 1 tasks\n});\nsize_t off2 = client.append({\n500\n, \n0.9\n, \n0.05\n, \nWARN: Server {2} down\n});\nsize_t off3 = client.append({\n1001\n, \n0.9\n, \n0.03\n, \nWARN: Server {2, 4, 5} down\n});\n\n\n\n\nQuerying data in the Atomic MultiLog is also similar to the Embedded mode API.\n\n\nIt is straightforward to retrieve records given their offsets:\n\n\nauto record1 = client.read(off1);\nauto record2 = client.read(off2);\nauto record3 = client.read(off3);\n\n\n\n\nWe can query indexed attributes as follows:\n\n\nauto record_stream = client.execute_filter(\ncpu_util\n0.5 || mem_avail\n0.5\n);\nfor (auto s = record_stream; !s.empty(); ++s) {\n  std::cout \n s.get().to_string();\n}\n\n\n\n\nWe can query a pre-defined filter as follows:\n\n\nauto record_stream = client.query_filter(\nlow_resources\n, 0, UINT64_MAX);\nfor (auto s = record_stream; !s.empty(); ++s) {\n  std::cout \n s.get().to_string();\n}\n\n\n\n\nWe can obtian the value of a pre-defined aggregate as follows:\n\n\nstd::string value = client.get_aggregate(\nmax_latency_ms\n, 0, UINT64_MAX);\nstd::cout \n value;\n\n\n\n\nFinally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:\n\n\nauto alert_stream = client.get_alerts(0, UINT64_MAX, \nhigh_latency_trigger\n);\nfor (auto s = alert_stream; !s.empty(); ++s) {\n  std::cout \n s.get();\n}", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick_start/#quick-start", 
            "text": "In this Quick Start, we will take a look at how to download and setup Confluo,\nload some sample data, and query it.", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quick_start/#pre-requisites", 
            "text": "MacOS X or Unix-based OS; Windows is not yet supported.  C++ compiler that supports C++11 standard  CMake 2.8 or later  Boost 1.53 or later   For python client, you will additionally require:   Python 2.7 or later  Python Packages: six 1.7.2 or later   For java client, you will additionally require:   Java 1.7 or later  ant 1.6.2 or later", 
            "title": "Pre-requisites"
        }, 
        {
            "location": "/quick_start/#download-and-install", 
            "text": "To download and install Confluo, use the following commands:  git clone https://github.com/ucbrise/confluo.git\ncd confluo\nmkdir build\ncd build\ncmake ..\nmake -j   make test   make install", 
            "title": "Download and Install"
        }, 
        {
            "location": "/quick_start/#using-confluo", 
            "text": "Confluo can be used in two modes -- embedded and stand-alone. In the embedded mode,\nConfluo is used as a header-only library in C++, allowing Confluo to use the same\naddress-space as the application process. In the stand-alone mode, Confluo runs\nas a daemon server process, allowing clients to communicate with it using  Apache Thrift  protocol.", 
            "title": "Using Confluo"
        }, 
        {
            "location": "/quick_start/#embedded-mode", 
            "text": "In order to use Confluo in the embedded mode, we simply need to include\nConfluo's header files under libconfluo/confluo, use the Confluo C++ API in \na C++ application, and compile using a modern C++ compiler. The entry point \nheader file to include is  confluo_store.h .   We will first create a new Confluo Store with the data path for it to use \nas follows:  confluo::confluo_store store( /path/to/data );  We then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); this requires three parameters: name, schema, and the storage mode:  std::string schema =  {\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n} ;\nauto storage_mode = confluo::storage::IN_MEMORY;\nstore.create_atomic_multilog( perf_log , schema, storage_mode);  Our schema contains 5 attributes: a signed 8-byte integer timestamp,\ndouble floating-point precision operation latency (in ms), CPU utilization\nand available memory, and a string log message field (upper bounded to 100 \ncharacters).   We then obtain a reference to our newly created Atomic MultiLog:  confluo::atomic_multilog* mlog = store.get_atomic_multilog( perf_log );  We can define indexes on the Atomic MultiLog as follows:  mlog- add_index( op_latency_ms );  to add an index on  op_latency_ms  attribute. We can also install filters as follows:  mlog- add_filter( low_resources ,  cpu_util 0.8 || mem_avail 0.1 );  to explicitly filter out records that indicate low system resources (CPU \nutilization   80%, Available Memory   10%), using a filter named  low_resources .  Additionally, we can add aggregates on filters as follows:  mlog- add_aggregate( max_latency_ms ,  low_resources ,  MAX(op_latency_ms) );  This adds a new stored aggregate  max_latency_ms  on the filter  low_resources  we defined before. In essence, it records the highest \noperation latency reported in any record that also indicated low \navailable resources.  Finally, we can install a trigger on aggregates as follows:  mlog- install_trigger( high_latency_trigger ,  max_latency   1000 );  This installs a trigger  high_latency_trigger  on the aggregate  max_latency_ms , which should generate an alert whenever the \ncondition  max_latency_ms   1000  is satisfied, i.e.,\nwhenever the maximum latency for an operation exceeds 1s and\nthe available resources are low.  We are now ready to load some data into this multilog:  size_t off1 = mlog- append({ 100 ,  0.5 ,  0.9 ,   INFO: Launched 1 tasks });\nsize_t off2 = mlog- append({ 500 ,  0.9 ,  0.05 ,  WARN: Server {2} down });\nsize_t off3 = mlog- append({ 1001 ,  0.9 ,  0.03 ,  WARN: Server {2, 4, 5} down });  Note that the  append  method takes a vector of strings as its input, where\nthe vector corresponds to a single record. The number of entries in the\nvector must match the number of entries in the schema, with the exception\nof the timestamp --- if the timestamp is not provided, Confluo will automatically\nassign one.  Also note that the operation returns a unique offset corresponding to each append\noperation. This forms the \"key\" for records stored in the Atomic MultiLog -- records\ncan be retrieved by specifying their corresponding offsets.  Now we take a look at how we can query the data in the Atomic MultiLog. First,\nit is straightforward to retrieve records given their offsets:  auto record1 = mlog- read(off1);\nauto record2 = mlog- read(off2);\nauto record3 = mlog- read(off3);  Each of  record1 ,  record2 , and  record3  are vectors of strings.  We can query indexed attributes as follows:  auto record_stream1 = mlog- execute_filter( cpu_util 0.5 || mem_avail 0.5 );\nfor (auto s = record_stream1; !s.empty(); s = s.tail()) {\n  std::cout   s.head().to_string();\n}  Note that the operation returns a lazily evaluated stream, which supports\nfunctional style operations like map, filter, etc. See  stream.h \nfor more details.  We can also query the defined filter as follows:  auto record_stream2 = mlog- query_filter( low_resources , 0, UINT64_MAX);\nfor (auto s = record_stream2; !s.empty(); s = s.tail()) {\n  std::cout   s.head().to_string();\n}  The first parameter corresponds to the name of the filter to be queried, while \nthe second and third parameters correspond to the begining timestamp and end \ntimestamp to consider for records in the filter. We've specified them to capture\nall possible values of timestamp. Similar to the  execute_filter  query, this\noperation also returns a lazily evaluated record stream.  We query aggregates as follows:  auto value = mlog- get_aggregate( max_latency_ms , 0, UINT64_MAX);\nstd::cout   value.to_string();  The query takes the name of the aggregate as its first parameter, while the \nsecond and third parameters correspond to begin and end timestmaps, as before.\nThe query returns a  numeric  \nobject, which is a wrapper around numeric values.  Finally, we can query the generated alerts by triggers we have installed as\nfollows:  auto alert_stream = mlog- get_alerts(0, UINT64_MAX,  high_latency_trigger );\nfor (auto s = alert_stream; !s.empty(); s = s.tail()) {\n  std::cout   s.head().to_string();\n}  The query takes and begin and end timestamps as its first and second arguments,\nand an optional trigger name as its third argument. The query returns a lazy \nstream over generated alerts for this trigger in the specified time-range.  See API docs for  C++  and in-depth user guides on  Data Storage \nand  Conflo Queries  for details on Confluo's supported operations.", 
            "title": "Embedded Mode"
        }, 
        {
            "location": "/quick_start/#stand-alone-mode", 
            "text": "In the stand-alone mode, Confluo runs as a daemon server, serving client requests\nusing Apache Thrift protocol. To start the server, run:  confuod --address=127.0.0.1 --port=9090  Once the server daemon is running, you can query it using the C++, Python or Java \nclient APIs. The client APIs closely resemble the embedded API. In this guide, \nwe will focus on the C++ client API, although client APIs for Python and Java are\nalmost identical.  We first create a new client connection to the Confluo daemon:  confluo::rpc::rpc_client client( 127.0.0.1 , 9090);  The first argument to the  rpc_client  constructor corresponds to the server\nhostname, while the second argument corresponds to the server port.  We then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); as before, this requires three parameters: a name for the Atomic \nMultiLog, a fixed schema, and a storage mode:  std::string schema =  {\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n} ;\nauto storage_mode = confluo::storage::IN_MEMORY;\nclient.create_atomic_multilog( perf_log , schema, storage_mode);  This operation also internally sets the current Atomic MultiLog \nfor the client to the one we just created (i.e.,  perf_log ).  We can define indexes as follows:  client.add_index( op_latency_ms );  We can also install filters as follows:  client.add_filter( low_resources ,  cpu_util 0.8 || mem_avail 0.1 );  Additionally, we can add aggregates on filters as follows:  client.add_aggregate( max_latency_ms ,  low_resources ,  MAX(op_latency_ms) );  Finally, we can install a trigger on an aggregate as follows:  client.install_trigger( high_latency_trigger ,  max_latency   1000 );  To load data into the Atomic MultiLog:  size_t off1 = client.append({ 100 ,  0.5 ,  0.9 ,   INFO: Launched 1 tasks });\nsize_t off2 = client.append({ 500 ,  0.9 ,  0.05 ,  WARN: Server {2} down });\nsize_t off3 = client.append({ 1001 ,  0.9 ,  0.03 ,  WARN: Server {2, 4, 5} down });  Querying data in the Atomic MultiLog is also similar to the Embedded mode API.  It is straightforward to retrieve records given their offsets:  auto record1 = client.read(off1);\nauto record2 = client.read(off2);\nauto record3 = client.read(off3);  We can query indexed attributes as follows:  auto record_stream = client.execute_filter( cpu_util 0.5 || mem_avail 0.5 );\nfor (auto s = record_stream; !s.empty(); ++s) {\n  std::cout   s.get().to_string();\n}  We can query a pre-defined filter as follows:  auto record_stream = client.query_filter( low_resources , 0, UINT64_MAX);\nfor (auto s = record_stream; !s.empty(); ++s) {\n  std::cout   s.get().to_string();\n}  We can obtian the value of a pre-defined aggregate as follows:  std::string value = client.get_aggregate( max_latency_ms , 0, UINT64_MAX);\nstd::cout   value;  Finally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:  auto alert_stream = client.get_alerts(0, UINT64_MAX,  high_latency_trigger );\nfor (auto s = alert_stream; !s.empty(); ++s) {\n  std::cout   s.get();\n}", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/install/", 
            "text": "Installation\n\n\nBefore you can install Confluo, make sure you have the following prerequisites:\n\n\n\n\nMacOS X or Unix-based OS; Windows is not yet supported.\n\n\nC++ compiler that supports C++11 standard\n\n\nCMake 2.8 or later\n\n\nBoost 1.53 or later\n\n\n\n\nFor python client, you will additionally require:\n\n\n\n\nPython 2.7 or later\n\n\nPython Packages: six 1.7.2 or later\n\n\n\n\nFor java client, you will additionally require:\n\n\n\n\nJava 1.7 or later\n\n\nant 1.6.2 or later\n\n\n\n\nDownload\n\n\nYou can obtain the latest version of Confluo by cloning the GitHub repository:\n\n\ngit clone https://github.com/ucbrise/confluo.git\n\n\n\n\nConfigure\n\n\nTo configure the build, Confluo uses CMake as its build system. Confluo only \nsupports out of source builds; the simplest way to configure the build would be \nas follows:\n\n\ncd confluo\nmkdir -p build\ncd build\ncmake ..\n\n\n\n\nIt is possible to configure the build specifying certain options based on \nrequirements; the supported options are:\n\n\n\n\nBUILD_TESTS\n: Builds all tests (ON by default)\n\n\nBUILD_RPC\n: Builds the rpc daemon and client libraries (ON by default)\n\n\nBUILD_EXAMPLES\n: Builds Confluo examples (ON by default)\n\n\nBUILD_DOC\n: Builds Confluo documentation (OFF by default)\n\n\nWITH_PY_CLIENT\n: Builds Confluo python rpc client (ON by default)\n\n\nWITH_JAVA_CLIENT\n: Builds Confluo java rpc client (ON by default)\n\n\n\n\nIn order to explicitly enable or disable any of these options, set the value of\nthe corresponding variable to \nON\n or \nOFF\n as follows:\n\n\ncmake -DBUILD_TESTS=OFF\n\n\n\n\nFinally, you can configure the install location for Confluo by modifying the\n\nCMAKE_INSTALL_PREFIX\n variable (which is set to /usr/local by default):\n\n\ncmake -DCMAKE_INSTALL_PREFIX=/path/to/installation\n\n\n\n\nInstall\n\n\nOnce the build is configured, you can proceed to compile, test and install \nConfluo. \n\n\nTo build, use:\n\n\nmake\n\n\n\n\nor \n\n\nmake -j{NUM_CORES}\n\n\n\n\nto speed up the build on multi-core systems.\n\n\nTo run the various unit tests, run:\n\n\nmake test\n\n\n\n\nand finally, to install, use:\n\n\nmake install", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installation", 
            "text": "Before you can install Confluo, make sure you have the following prerequisites:   MacOS X or Unix-based OS; Windows is not yet supported.  C++ compiler that supports C++11 standard  CMake 2.8 or later  Boost 1.53 or later   For python client, you will additionally require:   Python 2.7 or later  Python Packages: six 1.7.2 or later   For java client, you will additionally require:   Java 1.7 or later  ant 1.6.2 or later", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#download", 
            "text": "You can obtain the latest version of Confluo by cloning the GitHub repository:  git clone https://github.com/ucbrise/confluo.git", 
            "title": "Download"
        }, 
        {
            "location": "/install/#configure", 
            "text": "To configure the build, Confluo uses CMake as its build system. Confluo only \nsupports out of source builds; the simplest way to configure the build would be \nas follows:  cd confluo\nmkdir -p build\ncd build\ncmake ..  It is possible to configure the build specifying certain options based on \nrequirements; the supported options are:   BUILD_TESTS : Builds all tests (ON by default)  BUILD_RPC : Builds the rpc daemon and client libraries (ON by default)  BUILD_EXAMPLES : Builds Confluo examples (ON by default)  BUILD_DOC : Builds Confluo documentation (OFF by default)  WITH_PY_CLIENT : Builds Confluo python rpc client (ON by default)  WITH_JAVA_CLIENT : Builds Confluo java rpc client (ON by default)   In order to explicitly enable or disable any of these options, set the value of\nthe corresponding variable to  ON  or  OFF  as follows:  cmake -DBUILD_TESTS=OFF  Finally, you can configure the install location for Confluo by modifying the CMAKE_INSTALL_PREFIX  variable (which is set to /usr/local by default):  cmake -DCMAKE_INSTALL_PREFIX=/path/to/installation", 
            "title": "Configure"
        }, 
        {
            "location": "/install/#install", 
            "text": "Once the build is configured, you can proceed to compile, test and install \nConfluo.   To build, use:  make  or   make -j{NUM_CORES}  to speed up the build on multi-core systems.  To run the various unit tests, run:  make test  and finally, to install, use:  make install", 
            "title": "Install"
        }, 
        {
            "location": "/modes_of_operation/", 
            "text": "Modes of Operation\n\n\nConfluo can be used in two modes -- embedded and stand-alone. \n\n\nEmbedded Mode\n\n\nIn the \nembedded mode\n, Confluo is used as a header-only library in C++, allowing Confluo\nto use the same address-space as the application process. This enables ultra low-latency \nwrites and queries, but only supports applications written in C++.\n\n\nStand-alone Mode\n\n\nConfluo also supports a \nstand-alone mode\n, where Confluo runs as a daemon \nserver process and allows clients to communicate with it using \n\nApache Thrift\n protocol. Operations now incur higher latencies\n(due to serialization/deserialization overheads), but can now operate over the network, \nand allows Confluo to store data from applications written in different languages.\n\n\nMore on Usage\n\n\nRead more on how you can perform different operations with the two modes of operation:\n\n\n\n\nData Storage and Loading Data\n\n\nQuerying Data\n\n\nOnline Queries\n\n\nOffline Queries", 
            "title": "Modes of Operation"
        }, 
        {
            "location": "/modes_of_operation/#modes-of-operation", 
            "text": "Confluo can be used in two modes -- embedded and stand-alone.", 
            "title": "Modes of Operation"
        }, 
        {
            "location": "/modes_of_operation/#embedded-mode", 
            "text": "In the  embedded mode , Confluo is used as a header-only library in C++, allowing Confluo\nto use the same address-space as the application process. This enables ultra low-latency \nwrites and queries, but only supports applications written in C++.", 
            "title": "Embedded Mode"
        }, 
        {
            "location": "/modes_of_operation/#stand-alone-mode", 
            "text": "Confluo also supports a  stand-alone mode , where Confluo runs as a daemon \nserver process and allows clients to communicate with it using  Apache Thrift  protocol. Operations now incur higher latencies\n(due to serialization/deserialization overheads), but can now operate over the network, \nand allows Confluo to store data from applications written in different languages.", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/modes_of_operation/#more-on-usage", 
            "text": "Read more on how you can perform different operations with the two modes of operation:   Data Storage and Loading Data  Querying Data  Online Queries  Offline Queries", 
            "title": "More on Usage"
        }, 
        {
            "location": "/loading_data/", 
            "text": "Data Storage\n\n\nConfluo operates on \ndata streams\n. Each stream comprises of records, each of\nwhich follows a pre-defined schema over a collection of strongly-typed \nattributes.\n\n\nAttributes\n\n\nConfluo currently supports only \nbounded-width\n attributes\n1\n. An attribute \nis said to have bounded-width if all records in a single stream use some \nmaximum number of bits to represent that attribute; this includes primitive \ndata types such as binary, integral or floating-point values, or \ndomain-specific types such as IP addresses, ports, sensor readings, etc. \nConfluo also requires each record in the stream to have a 8-byte nano-second \nprecision timestamp attribute; if the application does not assign timestamps, \nConfluo internally assigns one during the write operation. \n\n\nSchema\n\n\nA schema in Confluo is a collection of strongly-typed attributes. It is specified\nvia JSON like semantics; for instance, consider the example below for a simple schema \nwith three attributes: \n\n\n{\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n}\n\n\n\n\nThe first attribute is timestamp with a 8-byte signed integer type; the second, third and\nfourth attributes correspond to operation latency (in ms), CPU utilization and available \nmemory respectively, all with double precision floating-point type. The final attribute is a\nlog message, with a string type upper bound by 100 characters. Note that each of the \nattributes must have types associated with them, and each record in a\nstream with this schema must have its attributes in this order. While Confluo natively \nsupports common primitive types, you can add custom bounded-width data types to Confluo's\ntype system. More details can be found at the \nConfluo Type-System\n guide.\n\n\nAtomic MultiLog\n\n\nAtomic MultiLogs are the basic storage abstraction in Confluo, and are similar in\ninterface to database tables. In order to store data from different streams, \napplications can create an Atomic MultiLog with a pre-specified schema, \nand write data streams that conform to the schema to the Atomic MultiLog.\n\n\nTo support queries, applications can add an index for individual attributes in the schema. \nConfluo also employs a match-action language with three main elements: \nfilter\n, \n\naggregate\n  and \ntrigger\n. \n\n\n\n\nA Confluo filter is an expression comprising of relational and boolean operators \n(see Table below) over arbitrary subset of bounded-width attributes, and identifies records \nthat match the expression. \n\n\nA Confluo aggregate evaluates a computable function on an attribute for all records that \nmatch a certain filter expression. \n\n\nFinally, a Confluo trigger is a boolean conditional (e.g., \n, \n, =, etc.) evaluated over \na Confluo aggregate. \n\n\n\n\nRelational Operators in Filters\n:\n\n\n\n\n\n\n\n\nOperator\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nEquality\n\n\ndst_port=80\n\n\n\n\n\n\nRange\n\n\ncpu_util\n0.8\n\n\n\n\n\n\n\n\nBoolean Operators in Filters\n:\n\n\n\n\n\n\n\n\nOperator\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nConjunction\n\n\nvolt\n200 \n temp\n100\n\n\n\n\n\n\nDisjunction\n\n\ncpu_util\n0.8 || mem_avail\n0.1\n\n\n\n\n\n\nNegation\n\n\ntransport_protocol != TCP\n\n\n\n\n\n\n\n\nConfluo supports indexes, filters, aggregates and triggers only on bounded-width\nattributes in the schema. Once added, each of these are evaluated and updated \nupon arrival of each new batch of data records.\n\n\nA Performance Monitoring and Diagnosis Example\n\n\nWe will now see how we can create Atomic MultiLogs, add indexes, filters, aggregate and triggers on them,\nand finally load some data into them, for both embedded and stand-alone modes of operation. We will work\nwith the example of a performance monitoring and diagnosis tool using Confluo.\n\n\nEmbedded mode\n\n\nIn order to use Confluo in the embedded mode, we simply need to include\nConfluo's header files under libconfluo/confluo, use the Confluo C++ API in \na C++ application, and compile using a modern C++ compiler. The entry point \nheader file to include is \nconfluo_store.h\n. \n\n\nCreating a New Confluo Store\n\n\nWe will first create a new Confluo Store with the data path for it to use \nas follows:\n\n\nconfluo::confluo_store store(\n/path/to/data\n);\n\n\n\n\nCreating a New Atomic MultiLog\n\n\nWe then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); this requires three parameters: a name for the Atomic MultiLog, a fixed\nschema, and a storage mode:\n\n\nstd::string schema = \n{\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n}\n;\nauto storage_mode = confluo::storage::IN_MEMORY;\nstore.create_atomic_multilog(\nperf_log\n, schema, storage_mode);\n\n\n\n\nOur Atomic MultiLog adopts the same schema outlined \nabove\n. The \nstorage mode is set to in-memory, but can be of the following types:\n\n\n\n\n\n\n\n\nStorage Mode\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nIN_MEMORY\n\n\nAll data is written purely in memory, and no attempt is made at persisting data to secondary storage.\n\n\n\n\n\n\nDURABLE\n\n\nOnly the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage for each write. The write is not considered complete unless its effects have been persisted to secondary storage.\n\n\n\n\n\n\nDURABLE_RELAXED\n\n\nOnly the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage; however, the data is buffered in memory and only persisted periodically, instead of persisting data for every write. This generally leads to better write performance.\n\n\n\n\n\n\n\n\nWe then obtain a reference to our newly created Atomic MultiLog:\n\n\nconfluo::atomic_multilog* mlog = store.get_atomic_multilog(\nperf_log\n);\n\n\n\n\nAdding Indexes\n\n\nWe can define indexes on the Atomic MultiLog as follows:\n\n\nmlog-\nadd_index(\nop_latency_ms\n);\n\n\n\n\nto add an index on \nop_latency_ms\n attribute. \n\n\nAdding Filters\n\n\nWe can also install filters as follows:\n\n\nmlog-\nadd_filter(\nlow_resources\n, \ncpu_util\n0.8 || mem_avail\n0.1\n);\n\n\n\n\nto explicitly filter out records that indicate low system resources (CPU \nutilization \n 80%, Available Memory \n 10%), using a filter named \nlow_resources\n. \n\n\nAdding Aggregates\n\n\nAdditionally, we can add aggregates on filters as follows:\n\n\nmlog-\nadd_aggregate(\nmax_latency_ms\n, \nlow_resources\n, \nMAX(op_latency_ms)\n);\n\n\n\n\nThis adds a new stored aggregate \nmax_latency_ms\n on the filter \n\nlow_resources\n we defined before. In essence, it records the highest \noperation latency reported in any record that also indicated low \navailable resources.\n\n\nInstalling Triggers\n\n\nFinally, we can install a trigger on aggregates as follows:\n\n\nmlog-\ninstall_trigger(\nhigh_latency_trigger\n, \nmax_latency \n 1000\n);\n\n\n\n\nThis installs a trigger \nhigh_latency_trigger\n on the aggregate \n\nmax_latency_ms\n, which should generate an alert whenever the \ncondition \nmax_latency_ms \n 1000\n is satisfied, i.e.,\nwhenever the maximum latency for an operation exceeds 1s and\nthe available resources are low.\n\n\nLoading sample data into Atomic MultiLog\n\n\nWe are now ready to load some data into this Atomic MultiLog. Atomic MutliLogs\nonly support addition of new data via \nappends\n. However, new data can be\nappended in several ways:\n\n\nAppending String Vectors\n\n\nThis version of \nappend\n method takes a vector of strings as its input, where\nthe vector corresponds to a single record. The number of entries in the\nvector must match the number of entries in the schema, with the exception\nof the timestamp --- if the timestamp is not provided, Confluo will automatically\nassign one.\n\n\nsize_t off1 = mlog-\nappend({\n100\n, \n0.5\n, \n0.9\n,  \nINFO: Launched 1 tasks\n});\nsize_t off2 = mlog-\nappend({\n500\n, \n0.9\n, \n0.05\n, \nWARN: Server {2} down\n});\nsize_t off3 = mlog-\nappend({\n1001\n, \n0.9\n, \n0.03\n, \nWARN: Server {2, 4, 5} down\n});\n\n\n\n\nAlso note that the operation returns a unique offset corresponding to each append\noperation. This forms the \"key\" for records stored in the Atomic MultiLog -- records\ncan be retrieved by specifying their corresponding offsets.\n\n\nAppending Raw bytes\n\n\nThis version of \nappend\n takes as its input a pointer to a C/C++ struct, that maps\nexactly to the Atomic MultiLog's schema. For instance, our schema would map to the following\nC/C++ struct:\n\n\nstruct perf_log_record {\n  int64_t timestamp;\n  double op_latency_ms;\n  double cpu_util;\n  double mem_avail;\n  char log_msg[100];\n};\n\n\n\n\nNote that \nlog_msg\n maps to a \nchar[100]\n rather than an \nstd::string\n. To add a new record, we\nwould populate a struct instance, and pass its reference to the append function:\n\n\nint64_t ts = utils::time_utils::cur_ns();\nperf_log_record rec = { ts, 2000.0, 0.95, 0.01, \nWARN: Server {2, 4, 5} down\n };\nsize_t off4 = mlog-\nappend(\nrec);\n\n\n\n\nNote that this is a more efficient variant of append, since it avoids the overheads of parsing \nstrings to the corresponding attribute data types.\n\n\nBatched Appends\n\n\nIt is also possible to batch multiple record appends into a single append. The first\nstep in building a batch is to obtain a batch builder:\n\n\nauto batch_bldr = mlog-\nget_batch_builder();\n\n\n\n\nThe batch builder supports adding new records via both string vector and raw byte interfaces:\n\n\nbatch_bldr.add_record({ \n400\n, \n0.85\n, \n0.07\n, \nWARN: Server {2, 4} down\n});\nperf_log_record rec = { utils::time_utils::cur_ns(), 100.0, 0.65, 0.25, \nWARN: Server {2} down\n };\nbatch_bldr.add_record(\nrec);\n\n\n\n\nOnce the batch is populated, we can append the batch to the Atomic MultiLog as follows:\n\n\nsize_t off5 = mlog-\nappend_batch(batch_bldr.get_batch());\n\n\n\n\nTo understand how we can query the data we have loaded so far, read the guide on \nConfluo Queries\n.\n\n\nStand-alone Mode\n\n\nIn the stand-alone mode, Confluo runs as a daemon server, serving client requests\nusing Apache Thrift protocol. To start the server, run:\n\n\nconfuod --address=127.0.0.1 --port=9090\n\n\n\n\nOnce the server daemon is running, you can send requests to it using the \nC++/Python/Java client APIs. We will focus on the C++ Client API, although \nthe Python/Java API is almost identical. In fact, even the C++ Client API is \nalmost identical to the embedded mode API.\n\n\nWe look at the same performance monitoring and diagnosis tool example for the\nstand-alone mode. The relevant header file to include for the C++ Client API is\n\nrpc_client.h\n.\n\n\nCreating a Client Connection\n\n\nTo begin with, we first have to establish a client connection with the server.\n\n\nconfluo::rpc::rpc_client client(\n127.0.0.1\n, 9090);\n\n\n\n\nThe first argument to the \nrpc_client\n constructor corresponds to the server\nhostname, while the second argument corresponds to the server port.\n\n\nCreating a New Atomic MultiLog\n\n\nWe then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); as before, this requires three parameters: a name for the Atomic \nMultiLog, a fixed schema, and a storage mode:\n\n\nstd::string schema = \n{\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n}\n;\nauto storage_mode = confluo::storage::IN_MEMORY;\nclient.create_atomic_multilog(\nperf_log\n, schema, storage_mode);\n\n\n\n\nThis operation also internally sets the current Atomic MultiLog \nfor the client to the one we just created (i.e., \nperf_log\n). It\nis also possible to explicitly set the current Atomic MultiLog for\nthe client as follows:\n\n\nclient.set_current_atomic_multilog(\nperf_log\n);\n\n\n\n\n\n\nNote\n\n\nIt is necessary to set the current Atomic MultiLog for the \nrpc_client\n;\nissuing requests via the client without setting the current Atomic MultiLog\nwill result in exceptions.\n\n\n\n\nAdding Indexes\n\n\nWe can define indexes as follows:\n\n\nclient.add_index(\nop_latency_ms\n);\n\n\n\n\nAdding Filters\n\n\nWe can also install filters as follows:\n\n\nclient.add_filter(\nlow_resources\n, \ncpu_util\n0.8 || mem_avail\n0.1\n);\n\n\n\n\nAdding Aggregates\n\n\nAdditionally, we can add aggregates on filters as follows:\n\n\nclient.add_aggregate(\nmax_latency_ms\n, \nlow_resources\n, \nMAX(op_latency_ms)\n);\n\n\n\n\nInstalling Triggers\n\n\nFinally, we can install a trigger on an aggregate as follows:\n\n\nclient.install_trigger(\nhigh_latency_trigger\n, \nmax_latency \n 1000\n);\n\n\n\n\nLoading sample data into Atomic MultiLog\n\n\nWe are now ready to load some data into the Atomic MultiLog on the server. \n\n\nAppending String Vectors\n\n\nsize_t off1 = client.append({\n100\n, \n0.5\n, \n0.9\n,  \nINFO: Launched 1 tasks\n});\nsize_t off2 = client.append({\n500\n, \n0.9\n, \n0.05\n, \nWARN: Server {2} down\n});\nsize_t off3 = client.append({\n1001\n, \n0.9\n, \n0.03\n, \nWARN: Server {2, 4, 5} down\n});\n\n\n\n\nAppending Raw bytes\n\n\nAs with the embedded mode, this version of \nappend\n takes as its input a pointer to a C/C++ struct that maps\n\nexactly\n to the Atomic MultiLog's schema. Our schema would map to the following C/C++ struct:\n\n\nstruct perf_log_record {\n  int64_t timestamp;\n  double op_latency_ms;\n  double cpu_util;\n  double mem_avail;\n  char log_msg[100];\n};\n\n\n\n\nUnlike the embedded interface, to add a new record, we now have to wrap the struct reference in a \nrecord_data\n object:\n\n\nint64_t ts = utils::time_utils::cur_ns();\nperf_log_record rec = { ts, 2000.0, 0.95, 0.01, \nWARN: Server {2, 4, 5} down\n };\nsize_t off4 = client.append(confluo::rpc::record_data(\nrec, sizeof(rec)));\n\n\n\n\nAs before, this is a more efficient variant of append, since it avoids the overheads of parsing \nstrings to the corresponding attribute data types.\n\n\nBatched Appends\n\n\nIt is also possible to batch multiple record appends into a single append operation via the client API. \nThis is particularly useful since batching helps amortize the cost of network latency.\n\n\nThe first step in building a batch is to obtain a batch builder:\n\n\nauto batch_bldr = client.get_batch_builder();\n\n\n\n\nThe batch builder supports adding new records via both string vector and raw byte interfaces:\n\n\nbatch_bldr.add_record({ \n400\n, \n0.85\n, \n0.07\n, \nWARN: Server {2, 4} down\n});\nperf_log_record rec = { utils::time_utils::cur_ns(), 100.0, 0.65, 0.25, \nWARN: Server {2} down\n };\nbatch_bldr.add_record(confluo::rpc::record_data(\nrec, sizeof(rec)));\n\n\n\n\nOnce the batch is populated, we can append the batch as follows:\n\n\nsize_t off5 = client.append_batch(batch_bldr.get_batch());\n\n\n\n\nDetails on querying the data via the client interface can be found in the guide on \nConfluo Queries\n.\n\n\n\n\n\n\n\n\n\n\nWe plan on adding support for variable width data types in a future relase.", 
            "title": "Data Storage Overview"
        }, 
        {
            "location": "/loading_data/#data-storage", 
            "text": "Confluo operates on  data streams . Each stream comprises of records, each of\nwhich follows a pre-defined schema over a collection of strongly-typed \nattributes.", 
            "title": "Data Storage"
        }, 
        {
            "location": "/loading_data/#attributes", 
            "text": "Confluo currently supports only  bounded-width  attributes 1 . An attribute \nis said to have bounded-width if all records in a single stream use some \nmaximum number of bits to represent that attribute; this includes primitive \ndata types such as binary, integral or floating-point values, or \ndomain-specific types such as IP addresses, ports, sensor readings, etc. \nConfluo also requires each record in the stream to have a 8-byte nano-second \nprecision timestamp attribute; if the application does not assign timestamps, \nConfluo internally assigns one during the write operation.", 
            "title": "Attributes"
        }, 
        {
            "location": "/loading_data/#schema", 
            "text": "A schema in Confluo is a collection of strongly-typed attributes. It is specified\nvia JSON like semantics; for instance, consider the example below for a simple schema \nwith three attributes:   {\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n}  The first attribute is timestamp with a 8-byte signed integer type; the second, third and\nfourth attributes correspond to operation latency (in ms), CPU utilization and available \nmemory respectively, all with double precision floating-point type. The final attribute is a\nlog message, with a string type upper bound by 100 characters. Note that each of the \nattributes must have types associated with them, and each record in a\nstream with this schema must have its attributes in this order. While Confluo natively \nsupports common primitive types, you can add custom bounded-width data types to Confluo's\ntype system. More details can be found at the  Confluo Type-System  guide.", 
            "title": "Schema"
        }, 
        {
            "location": "/loading_data/#atomic-multilog", 
            "text": "Atomic MultiLogs are the basic storage abstraction in Confluo, and are similar in\ninterface to database tables. In order to store data from different streams, \napplications can create an Atomic MultiLog with a pre-specified schema, \nand write data streams that conform to the schema to the Atomic MultiLog.  To support queries, applications can add an index for individual attributes in the schema. \nConfluo also employs a match-action language with three main elements:  filter ,  aggregate   and  trigger .    A Confluo filter is an expression comprising of relational and boolean operators \n(see Table below) over arbitrary subset of bounded-width attributes, and identifies records \nthat match the expression.   A Confluo aggregate evaluates a computable function on an attribute for all records that \nmatch a certain filter expression.   Finally, a Confluo trigger is a boolean conditional (e.g.,  ,  , =, etc.) evaluated over \na Confluo aggregate.    Relational Operators in Filters :     Operator  Examples      Equality  dst_port=80    Range  cpu_util 0.8     Boolean Operators in Filters :     Operator  Examples      Conjunction  volt 200   temp 100    Disjunction  cpu_util 0.8 || mem_avail 0.1    Negation  transport_protocol != TCP     Confluo supports indexes, filters, aggregates and triggers only on bounded-width\nattributes in the schema. Once added, each of these are evaluated and updated \nupon arrival of each new batch of data records.", 
            "title": "Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#a-performance-monitoring-and-diagnosis-example", 
            "text": "We will now see how we can create Atomic MultiLogs, add indexes, filters, aggregate and triggers on them,\nand finally load some data into them, for both embedded and stand-alone modes of operation. We will work\nwith the example of a performance monitoring and diagnosis tool using Confluo.", 
            "title": "A Performance Monitoring and Diagnosis Example"
        }, 
        {
            "location": "/loading_data/#embedded-mode", 
            "text": "In order to use Confluo in the embedded mode, we simply need to include\nConfluo's header files under libconfluo/confluo, use the Confluo C++ API in \na C++ application, and compile using a modern C++ compiler. The entry point \nheader file to include is  confluo_store.h .", 
            "title": "Embedded mode"
        }, 
        {
            "location": "/loading_data/#creating-a-new-confluo-store", 
            "text": "We will first create a new Confluo Store with the data path for it to use \nas follows:  confluo::confluo_store store( /path/to/data );", 
            "title": "Creating a New Confluo Store"
        }, 
        {
            "location": "/loading_data/#creating-a-new-atomic-multilog", 
            "text": "We then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); this requires three parameters: a name for the Atomic MultiLog, a fixed\nschema, and a storage mode:  std::string schema =  {\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n} ;\nauto storage_mode = confluo::storage::IN_MEMORY;\nstore.create_atomic_multilog( perf_log , schema, storage_mode);  Our Atomic MultiLog adopts the same schema outlined  above . The \nstorage mode is set to in-memory, but can be of the following types:     Storage Mode  Description      IN_MEMORY  All data is written purely in memory, and no attempt is made at persisting data to secondary storage.    DURABLE  Only the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage for each write. The write is not considered complete unless its effects have been persisted to secondary storage.    DURABLE_RELAXED  Only the raw data (i.e., raw bytes corresponding to each record) is persisted to secondary storage; however, the data is buffered in memory and only persisted periodically, instead of persisting data for every write. This generally leads to better write performance.     We then obtain a reference to our newly created Atomic MultiLog:  confluo::atomic_multilog* mlog = store.get_atomic_multilog( perf_log );", 
            "title": "Creating a New Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#adding-indexes", 
            "text": "We can define indexes on the Atomic MultiLog as follows:  mlog- add_index( op_latency_ms );  to add an index on  op_latency_ms  attribute.", 
            "title": "Adding Indexes"
        }, 
        {
            "location": "/loading_data/#adding-filters", 
            "text": "We can also install filters as follows:  mlog- add_filter( low_resources ,  cpu_util 0.8 || mem_avail 0.1 );  to explicitly filter out records that indicate low system resources (CPU \nutilization   80%, Available Memory   10%), using a filter named  low_resources .", 
            "title": "Adding Filters"
        }, 
        {
            "location": "/loading_data/#adding-aggregates", 
            "text": "Additionally, we can add aggregates on filters as follows:  mlog- add_aggregate( max_latency_ms ,  low_resources ,  MAX(op_latency_ms) );  This adds a new stored aggregate  max_latency_ms  on the filter  low_resources  we defined before. In essence, it records the highest \noperation latency reported in any record that also indicated low \navailable resources.", 
            "title": "Adding Aggregates"
        }, 
        {
            "location": "/loading_data/#installing-triggers", 
            "text": "Finally, we can install a trigger on aggregates as follows:  mlog- install_trigger( high_latency_trigger ,  max_latency   1000 );  This installs a trigger  high_latency_trigger  on the aggregate  max_latency_ms , which should generate an alert whenever the \ncondition  max_latency_ms   1000  is satisfied, i.e.,\nwhenever the maximum latency for an operation exceeds 1s and\nthe available resources are low.", 
            "title": "Installing Triggers"
        }, 
        {
            "location": "/loading_data/#loading-sample-data-into-atomic-multilog", 
            "text": "We are now ready to load some data into this Atomic MultiLog. Atomic MutliLogs\nonly support addition of new data via  appends . However, new data can be\nappended in several ways:", 
            "title": "Loading sample data into Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#appending-string-vectors", 
            "text": "This version of  append  method takes a vector of strings as its input, where\nthe vector corresponds to a single record. The number of entries in the\nvector must match the number of entries in the schema, with the exception\nof the timestamp --- if the timestamp is not provided, Confluo will automatically\nassign one.  size_t off1 = mlog- append({ 100 ,  0.5 ,  0.9 ,   INFO: Launched 1 tasks });\nsize_t off2 = mlog- append({ 500 ,  0.9 ,  0.05 ,  WARN: Server {2} down });\nsize_t off3 = mlog- append({ 1001 ,  0.9 ,  0.03 ,  WARN: Server {2, 4, 5} down });  Also note that the operation returns a unique offset corresponding to each append\noperation. This forms the \"key\" for records stored in the Atomic MultiLog -- records\ncan be retrieved by specifying their corresponding offsets.", 
            "title": "Appending String Vectors"
        }, 
        {
            "location": "/loading_data/#appending-raw-bytes", 
            "text": "This version of  append  takes as its input a pointer to a C/C++ struct, that maps\nexactly to the Atomic MultiLog's schema. For instance, our schema would map to the following\nC/C++ struct:  struct perf_log_record {\n  int64_t timestamp;\n  double op_latency_ms;\n  double cpu_util;\n  double mem_avail;\n  char log_msg[100];\n};  Note that  log_msg  maps to a  char[100]  rather than an  std::string . To add a new record, we\nwould populate a struct instance, and pass its reference to the append function:  int64_t ts = utils::time_utils::cur_ns();\nperf_log_record rec = { ts, 2000.0, 0.95, 0.01,  WARN: Server {2, 4, 5} down  };\nsize_t off4 = mlog- append( rec);  Note that this is a more efficient variant of append, since it avoids the overheads of parsing \nstrings to the corresponding attribute data types.", 
            "title": "Appending Raw bytes"
        }, 
        {
            "location": "/loading_data/#batched-appends", 
            "text": "It is also possible to batch multiple record appends into a single append. The first\nstep in building a batch is to obtain a batch builder:  auto batch_bldr = mlog- get_batch_builder();  The batch builder supports adding new records via both string vector and raw byte interfaces:  batch_bldr.add_record({  400 ,  0.85 ,  0.07 ,  WARN: Server {2, 4} down });\nperf_log_record rec = { utils::time_utils::cur_ns(), 100.0, 0.65, 0.25,  WARN: Server {2} down  };\nbatch_bldr.add_record( rec);  Once the batch is populated, we can append the batch to the Atomic MultiLog as follows:  size_t off5 = mlog- append_batch(batch_bldr.get_batch());  To understand how we can query the data we have loaded so far, read the guide on  Confluo Queries .", 
            "title": "Batched Appends"
        }, 
        {
            "location": "/loading_data/#stand-alone-mode", 
            "text": "In the stand-alone mode, Confluo runs as a daemon server, serving client requests\nusing Apache Thrift protocol. To start the server, run:  confuod --address=127.0.0.1 --port=9090  Once the server daemon is running, you can send requests to it using the \nC++/Python/Java client APIs. We will focus on the C++ Client API, although \nthe Python/Java API is almost identical. In fact, even the C++ Client API is \nalmost identical to the embedded mode API.  We look at the same performance monitoring and diagnosis tool example for the\nstand-alone mode. The relevant header file to include for the C++ Client API is rpc_client.h .", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/loading_data/#creating-a-client-connection", 
            "text": "To begin with, we first have to establish a client connection with the server.  confluo::rpc::rpc_client client( 127.0.0.1 , 9090);  The first argument to the  rpc_client  constructor corresponds to the server\nhostname, while the second argument corresponds to the server port.", 
            "title": "Creating a Client Connection"
        }, 
        {
            "location": "/loading_data/#creating-a-new-atomic-multilog_1", 
            "text": "We then create a new Atomic MultiLog within the Store (synonymous to a database\ntable); as before, this requires three parameters: a name for the Atomic \nMultiLog, a fixed schema, and a storage mode:  std::string schema =  {\n  timestamp: LONG,\n  op_latency_ms: DOUBLE,\n  cpu_util: DOUBLE,\n  mem_avail: DOUBLE,\n  log_msg: STRING(100)\n} ;\nauto storage_mode = confluo::storage::IN_MEMORY;\nclient.create_atomic_multilog( perf_log , schema, storage_mode);  This operation also internally sets the current Atomic MultiLog \nfor the client to the one we just created (i.e.,  perf_log ). It\nis also possible to explicitly set the current Atomic MultiLog for\nthe client as follows:  client.set_current_atomic_multilog( perf_log );   Note  It is necessary to set the current Atomic MultiLog for the  rpc_client ;\nissuing requests via the client without setting the current Atomic MultiLog\nwill result in exceptions.", 
            "title": "Creating a New Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#adding-indexes_1", 
            "text": "We can define indexes as follows:  client.add_index( op_latency_ms );", 
            "title": "Adding Indexes"
        }, 
        {
            "location": "/loading_data/#adding-filters_1", 
            "text": "We can also install filters as follows:  client.add_filter( low_resources ,  cpu_util 0.8 || mem_avail 0.1 );", 
            "title": "Adding Filters"
        }, 
        {
            "location": "/loading_data/#adding-aggregates_1", 
            "text": "Additionally, we can add aggregates on filters as follows:  client.add_aggregate( max_latency_ms ,  low_resources ,  MAX(op_latency_ms) );", 
            "title": "Adding Aggregates"
        }, 
        {
            "location": "/loading_data/#installing-triggers_1", 
            "text": "Finally, we can install a trigger on an aggregate as follows:  client.install_trigger( high_latency_trigger ,  max_latency   1000 );", 
            "title": "Installing Triggers"
        }, 
        {
            "location": "/loading_data/#loading-sample-data-into-atomic-multilog_1", 
            "text": "We are now ready to load some data into the Atomic MultiLog on the server.", 
            "title": "Loading sample data into Atomic MultiLog"
        }, 
        {
            "location": "/loading_data/#appending-string-vectors_1", 
            "text": "size_t off1 = client.append({ 100 ,  0.5 ,  0.9 ,   INFO: Launched 1 tasks });\nsize_t off2 = client.append({ 500 ,  0.9 ,  0.05 ,  WARN: Server {2} down });\nsize_t off3 = client.append({ 1001 ,  0.9 ,  0.03 ,  WARN: Server {2, 4, 5} down });", 
            "title": "Appending String Vectors"
        }, 
        {
            "location": "/loading_data/#appending-raw-bytes_1", 
            "text": "As with the embedded mode, this version of  append  takes as its input a pointer to a C/C++ struct that maps exactly  to the Atomic MultiLog's schema. Our schema would map to the following C/C++ struct:  struct perf_log_record {\n  int64_t timestamp;\n  double op_latency_ms;\n  double cpu_util;\n  double mem_avail;\n  char log_msg[100];\n};  Unlike the embedded interface, to add a new record, we now have to wrap the struct reference in a  record_data  object:  int64_t ts = utils::time_utils::cur_ns();\nperf_log_record rec = { ts, 2000.0, 0.95, 0.01,  WARN: Server {2, 4, 5} down  };\nsize_t off4 = client.append(confluo::rpc::record_data( rec, sizeof(rec)));  As before, this is a more efficient variant of append, since it avoids the overheads of parsing \nstrings to the corresponding attribute data types.", 
            "title": "Appending Raw bytes"
        }, 
        {
            "location": "/loading_data/#batched-appends_1", 
            "text": "It is also possible to batch multiple record appends into a single append operation via the client API. \nThis is particularly useful since batching helps amortize the cost of network latency.  The first step in building a batch is to obtain a batch builder:  auto batch_bldr = client.get_batch_builder();  The batch builder supports adding new records via both string vector and raw byte interfaces:  batch_bldr.add_record({  400 ,  0.85 ,  0.07 ,  WARN: Server {2, 4} down });\nperf_log_record rec = { utils::time_utils::cur_ns(), 100.0, 0.65, 0.25,  WARN: Server {2} down  };\nbatch_bldr.add_record(confluo::rpc::record_data( rec, sizeof(rec)));  Once the batch is populated, we can append the batch as follows:  size_t off5 = client.append_batch(batch_bldr.get_batch());  Details on querying the data via the client interface can be found in the guide on  Confluo Queries .      We plan on adding support for variable width data types in a future relase.", 
            "title": "Batched Appends"
        }, 
        {
            "location": "/type_system/", 
            "text": "Confluo Type System\n\n\nConfluo uses a strictly typed system. While primitive data types like\n\nBOOL\n, \nCHAR\n, \nSHORT\n, \nINT\n, \nLONG\n, \nFLOAT\n, \nDOUBLE\n and \nSTRING\n\nare supported by default in Confluo, it is possible to add custom \nuser-defined data types. This requires defining a few operations that would\nallow operations like applying filters and triggers on attributes of\nthe custom data type.\n\n\nRegistering Types\n\n\nTo create a new type, we need to define the following properties so that\nnative operations can be supported; these properties are summarised in the\n\ntype_properties\n \nstruct:\n\n\n\n\nstd::string name\n - A unique name for the type\n\n\nsize_t size\n -  The size of underlying representation for fixed sized types. This should be set to zero for \ndynamically sized types (e.g., see definintion for \nSTRING\n type).\n\n\nvoid* min\n - This is a pointer to the minimum value that the type\ncan hold. See \ntype_properties.h\n \nto see examples of \nmin\n assigned to primitive types.\n\n\nvoid* max\n - This is a pointer to the maximum value that the type\ncan hold. See \ntype_properties.h\n \nto see examples of \nmax\n assigned to primitive types.\n\n\nvoid* one\n - This is a pointer to the step value with which the type\ncan be incremented. See \ntype_properties.h\n \nto see examples of \none\n assigned to primitive types.\n\n\nvoid* zero\n - This is a pointer to the zero value for the type. See \n\ntype_properties.h\n to see \nexamples of \nzero\n assigned to primitive types.\n\n\nbool is_numeric\n - This indicates whether the type is numeric or not; \nnumeric types typically support most arithmetic operators; see \n[arithmetic_ops.h][../libconfluo/confluo/types/arithmetic_ops.h] for examples.\n\n\nrelational_ops_t relational_ops\n - Stores a list of relational operator functions\nfor the given type, so that operations like \nfilter\n can work. See \n\nrel_ops.h\n for examples of\nwhat relational functions can be defined.\n\n\nbinary_ops_t binary_ops\n - Stores a list of binary arithmetic operator functions\nfor the given type, so that operations like filter can accurately be applied\nto the type. Check \narithmetic_ops.h\n\nfor examples of binary operator functions that can be defined.\n\n\nunary_ops_t unary_ops\n - Stores a list of unary arithmetic operator functions\nfor the given type, so that operations like filter can work for the given\ntype. Check \narithmetic_ops.h\n for\nexamples of unary function operators that can be defined.\n\n\nkey_op_t key_transform_op\n - Stores the key-transform function. This function \nis important for looking up attributes of the type in an index; see \n\nkey_ops.h\n for example\ndefinitions of key_transform.\n\n\nparse_op_t parse_op\n - Parses data instance from a string representation of this type. See \n\nstring_ops.h\n for examples.\n\n\nto_string_op_t to_stirng_op\n - Converts data instance of the type to its string representation. See \n\nstring_ops.h\n for examples.\n\n\nserialize_op_t serialize_op\n - Serializes the underlying data representation of the type into raw bytes;\nsee \nserde_ops.h\n for examples.\n\n\ndeserialize_op_t deserialize_op\n - Reads the raw byte representation of the type and parses it to data;\nsee \nserde_ops.h\n for examples.\n\n\n\n\nExample declarations of user-defined types can be found at\n\nip_address.h\n and \n\nsize_type.h\n.\n\n\nOnce the properties for custom type is defined in the \ntype_properties\n struct, \nit needs to be registered with Confluo's \ntype manager\n \nvia the \ntype_manager::register_type\n interface. We can register a type as \nfollows:\n\n\ntype_properties test_properties(\ntest\n, sizeof(int), \nlimits::int_min,\n                                \nlimits::int_max, \nlimits::int_one, \n                                \nlimits::int_one, \nlimits::int_zero, false,\n                                get_relops(), get_unaryops(), \n                                get_binaryops(),\n                                get_keyops(), \ntest_type::parse_test,\n                                \ntest_type::test_to_string,\n                                \nconfluo::serialize\ntest\n,\n                                \nconfluo::deserialize\ntest\n);\ntype_manager::register_type(test_properties);\n\n\n\n\nOnce registered, a useful symbolic\nreference to the data type, wrapped in a \ndata_type\n object, can be obtained via\nthe \ntype_manager::get_type\n interface.\n\n\nWith this object, it is possible to add new columns of this type in any \nAtomic MultiLog. From here on out, appending records to the\nAtomic MultiLog, along with operations like filters and triggers, will work out\nof the box.\n\n\nBuilding a Schema\n\n\nWe can create columns of a custom user-defined type for an Atomic MultiLog\nas follows:\n\n\ndata_type test_type = type_manager::get_type(\ntest\n);\nschema_builder builder;\nbuilder.add_column(test_type, \na\n);\nstd::vector\ncolumn_t\n s = builder.get_columns();\n\ntask_pool MGMT_POOL;\natomic_multilog dtable(\ntable\n, s, \n/tmp\n, storage::IN_MEMORY, MGMT_POOL);\n\n\n\n\nWe use the \ntype_manager::get_type\n interface to get the \ndata_type\n object\nassociated with the new user-defined type. This object can then be passed\ninto the \nadd_column\n interface of the \nschema_builder\n along with the name\nof the column. Finally, the vector of columns can be passed into the Atomic\nMultilog and thus operations like filters and triggers can be performed on\ndata of the user-defined type.\n\n\nAdding Records\n\n\nFirst create a packed struct containing all of the member data_types \nin a row of the schema. Be sure to include the timestamp as one of \nthe columns. An example is as follows: \n\n\nstruct rec {\n    int64_t ts;\n    test_type a;\n}__attribute__((packed));\n\n\n\n\nThen get a pointer to an instance of the struct, which is passed into the\n\nappend\n method of the Atomic MultiLog. The following is an exmaple:\n\n\nrec r = {utils::time_utils::cur_ns(), test_type()};\nvoid* data = reinterpret_cast\nvoid*\n(\nr);\ndtable.append(r);\n\n\n\n\nHere we initialize a record with the current timestamp and an instance of\nthe test type. Then we pass a pointer to that data to the append method\nof the Atomic MultiLog instance.\n\n\nPerforming Filter Operations\n\n\nAfter adding records containing data of the user-defined type, we can \nperform filter operations to select specific records. We can do so as \nfollows:\n\n\nfor (auto r = dtable.execute_filter(\na \n 4\n); !r.empty(); r = r.tail()) {\n    std::cout \n \nA value: \n \n r.head().at(1).as\ntest_type\n().get_test()\n                                            \n std::endl;\n}\n\n\n\n\nWe assume here that \n4\n can be parsed as a \ntest_type\n using the \nparse_test method. The \nexecute_filter\n function is then called with an\nexpression that selects records that satisfy the condition. This returns\na stream of records which can then be read and processed.\n\n\nSee \ntype_manager_test.h\n for\nexamples of building user-defined types including ip address and size types.", 
            "title": "Type System"
        }, 
        {
            "location": "/type_system/#confluo-type-system", 
            "text": "Confluo uses a strictly typed system. While primitive data types like BOOL ,  CHAR ,  SHORT ,  INT ,  LONG ,  FLOAT ,  DOUBLE  and  STRING \nare supported by default in Confluo, it is possible to add custom \nuser-defined data types. This requires defining a few operations that would\nallow operations like applying filters and triggers on attributes of\nthe custom data type.", 
            "title": "Confluo Type System"
        }, 
        {
            "location": "/type_system/#registering-types", 
            "text": "To create a new type, we need to define the following properties so that\nnative operations can be supported; these properties are summarised in the type_properties  \nstruct:   std::string name  - A unique name for the type  size_t size  -  The size of underlying representation for fixed sized types. This should be set to zero for \ndynamically sized types (e.g., see definintion for  STRING  type).  void* min  - This is a pointer to the minimum value that the type\ncan hold. See  type_properties.h  \nto see examples of  min  assigned to primitive types.  void* max  - This is a pointer to the maximum value that the type\ncan hold. See  type_properties.h  \nto see examples of  max  assigned to primitive types.  void* one  - This is a pointer to the step value with which the type\ncan be incremented. See  type_properties.h  \nto see examples of  one  assigned to primitive types.  void* zero  - This is a pointer to the zero value for the type. See  type_properties.h  to see \nexamples of  zero  assigned to primitive types.  bool is_numeric  - This indicates whether the type is numeric or not; \nnumeric types typically support most arithmetic operators; see \n[arithmetic_ops.h][../libconfluo/confluo/types/arithmetic_ops.h] for examples.  relational_ops_t relational_ops  - Stores a list of relational operator functions\nfor the given type, so that operations like  filter  can work. See  rel_ops.h  for examples of\nwhat relational functions can be defined.  binary_ops_t binary_ops  - Stores a list of binary arithmetic operator functions\nfor the given type, so that operations like filter can accurately be applied\nto the type. Check  arithmetic_ops.h \nfor examples of binary operator functions that can be defined.  unary_ops_t unary_ops  - Stores a list of unary arithmetic operator functions\nfor the given type, so that operations like filter can work for the given\ntype. Check  arithmetic_ops.h  for\nexamples of unary function operators that can be defined.  key_op_t key_transform_op  - Stores the key-transform function. This function \nis important for looking up attributes of the type in an index; see  key_ops.h  for example\ndefinitions of key_transform.  parse_op_t parse_op  - Parses data instance from a string representation of this type. See  string_ops.h  for examples.  to_string_op_t to_stirng_op  - Converts data instance of the type to its string representation. See  string_ops.h  for examples.  serialize_op_t serialize_op  - Serializes the underlying data representation of the type into raw bytes;\nsee  serde_ops.h  for examples.  deserialize_op_t deserialize_op  - Reads the raw byte representation of the type and parses it to data;\nsee  serde_ops.h  for examples.   Example declarations of user-defined types can be found at ip_address.h  and  size_type.h .  Once the properties for custom type is defined in the  type_properties  struct, \nit needs to be registered with Confluo's  type manager  \nvia the  type_manager::register_type  interface. We can register a type as \nfollows:  type_properties test_properties( test , sizeof(int),  limits::int_min,\n                                 limits::int_max,  limits::int_one, \n                                 limits::int_one,  limits::int_zero, false,\n                                get_relops(), get_unaryops(), \n                                get_binaryops(),\n                                get_keyops(),  test_type::parse_test,\n                                 test_type::test_to_string,\n                                 confluo::serialize test ,\n                                 confluo::deserialize test );\ntype_manager::register_type(test_properties);  Once registered, a useful symbolic\nreference to the data type, wrapped in a  data_type  object, can be obtained via\nthe  type_manager::get_type  interface.  With this object, it is possible to add new columns of this type in any \nAtomic MultiLog. From here on out, appending records to the\nAtomic MultiLog, along with operations like filters and triggers, will work out\nof the box.", 
            "title": "Registering Types"
        }, 
        {
            "location": "/type_system/#building-a-schema", 
            "text": "We can create columns of a custom user-defined type for an Atomic MultiLog\nas follows:  data_type test_type = type_manager::get_type( test );\nschema_builder builder;\nbuilder.add_column(test_type,  a );\nstd::vector column_t  s = builder.get_columns();\n\ntask_pool MGMT_POOL;\natomic_multilog dtable( table , s,  /tmp , storage::IN_MEMORY, MGMT_POOL);  We use the  type_manager::get_type  interface to get the  data_type  object\nassociated with the new user-defined type. This object can then be passed\ninto the  add_column  interface of the  schema_builder  along with the name\nof the column. Finally, the vector of columns can be passed into the Atomic\nMultilog and thus operations like filters and triggers can be performed on\ndata of the user-defined type.", 
            "title": "Building a Schema"
        }, 
        {
            "location": "/type_system/#adding-records", 
            "text": "First create a packed struct containing all of the member data_types \nin a row of the schema. Be sure to include the timestamp as one of \nthe columns. An example is as follows:   struct rec {\n    int64_t ts;\n    test_type a;\n}__attribute__((packed));  Then get a pointer to an instance of the struct, which is passed into the append  method of the Atomic MultiLog. The following is an exmaple:  rec r = {utils::time_utils::cur_ns(), test_type()};\nvoid* data = reinterpret_cast void* ( r);\ndtable.append(r);  Here we initialize a record with the current timestamp and an instance of\nthe test type. Then we pass a pointer to that data to the append method\nof the Atomic MultiLog instance.", 
            "title": "Adding Records"
        }, 
        {
            "location": "/type_system/#performing-filter-operations", 
            "text": "After adding records containing data of the user-defined type, we can \nperform filter operations to select specific records. We can do so as \nfollows:  for (auto r = dtable.execute_filter( a   4 ); !r.empty(); r = r.tail()) {\n    std::cout    A value:     r.head().at(1).as test_type ().get_test()\n                                              std::endl;\n}  We assume here that  4  can be parsed as a  test_type  using the \nparse_test method. The  execute_filter  function is then called with an\nexpression that selects records that satisfy the condition. This returns\na stream of records which can then be read and processed.  See  type_manager_test.h  for\nexamples of building user-defined types including ip address and size types.", 
            "title": "Performing Filter Operations"
        }, 
        {
            "location": "/queries/", 
            "text": "Querying Data\n\n\nQueries in Confluo can either be \nonline\n or \noffline\n. Online queries are executed as new data records\nare written to an Atomic MultiLog, while offline queries are evaluated on already written records.\nIn essence, online queries are similar to continuous queries databases.\n\n\nIn order to support online and offline queries, Confluo makes use of indexes, filters, aggregates \nand triggers. The interface for adding these elements to an Atomic MultiLog was described in the \nguide on \nData Storage and Loading\n.\n\n\nTo see how Confluo supports online and offline queries, see the individual guides at:\n\n\n\n\nOnline Queries\n\n\nOffline Queries", 
            "title": "Queries Overview"
        }, 
        {
            "location": "/queries/#querying-data", 
            "text": "Queries in Confluo can either be  online  or  offline . Online queries are executed as new data records\nare written to an Atomic MultiLog, while offline queries are evaluated on already written records.\nIn essence, online queries are similar to continuous queries databases.  In order to support online and offline queries, Confluo makes use of indexes, filters, aggregates \nand triggers. The interface for adding these elements to an Atomic MultiLog was described in the \nguide on  Data Storage and Loading .  To see how Confluo supports online and offline queries, see the individual guides at:   Online Queries  Offline Queries", 
            "title": "Querying Data"
        }, 
        {
            "location": "/online_queries/", 
            "text": "Online Queries\n\n\nOnline queries on Confluo are executed automatically as new records are written \ninto an Atomic MultiLog. Therefore, these queries need to be pre-defined; the \nguide on \nData Storage\n describes how we can define filters,\naggregates and triggers for online evaluation. In this guide, we will look\nat how we can retrieve the results of these online queries.\n\n\nEmbedded Mode\n\n\nWe begin with the embedded mode of operation. We work with the assumption that\nthe Atomic MultiLog has already been created, and filters, aggregates and triggers\nhave been added to it as outlined in \nData Storage\n.\n\n\nTo start with, we have a reference to the Atomic MultiLog as follows:\n\n\nconfluo::atomic_multilog* mlog = store.get_atomic_multilog(\nperf_log\n);\n\n\n\n\nQuerying Pre-defined Filters\n\n\nWe can query a pre-defined filter as follows:\n\n\nauto record_stream = mlog-\nquery_filter(\nlow_resources\n, 0, UINT64_MAX);\nfor (auto s = record_stream; !s.empty(); s = s.tail()) {\n  std::cout \n s.head().to_string();\n}\n\n\n\n\nThe first parameter corresponds to the name of the filter to be queried, while \nthe second and third parameters correspond to the begining timestamp and end \ntimestamp to consider for records in the filter. We've specified them to capture\nall possible values of timestamp. This operation returns a lazily evaluated record \nstream which supports functional semantics such as filter, map, etc. See Confluo's\n\nStream API\n \nfor more details on how to work with lazy streams.\n\n\nObtaining Pre-defined Aggregates\n\n\nWe can obtian a pre-defined aggregate as follows:\n\n\nauto value = mlog-\nget_aggregate(\nmax_latency_ms\n, 0, UINT64_MAX);\nstd::cout \n value.to_string();\n\n\n\n\nThe operation takes the name of the aggregate as its first parameter, while the \nsecond and third parameters correspond to begin and end timestmaps, as with \npre-defined filters. The query returns a \n\nnumeric\n \nobject, which is a wrapper around numeric values in C++.\n\n\nObtaining Alerts from a Pre-defined Trigger\n\n\nFinally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:\n\n\nauto alert_stream = mlog-\nget_alerts(0, UINT64_MAX, \nhigh_latency_trigger\n);\nfor (auto s = alert_stream; !s.empty(); s = s.tail()) {\n  std::cout \n s.head().to_string();\n}\n\n\n\n\nThe query takes begin and end timestamps as its first and second arguments,\nand an optional trigger name as its third argument. The query returns a lazy \nstream over generated alerts for this trigger in the specified time-range.\n\n\nStand-alone Mode\n\n\nThe API for Stand-alone mode of operation is quite similar to the embedded mode.\nWe only focus on the C++ Client API, since Python and Java Client APIs are\nalmost identical to the C++ Client API.\n\n\nAs with the embedded mode, we work with the assumption that the client is connected to the server, \nhas already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers.\nAlso, the current Atomic MultiLog for the client has been set to \nperf_log\n as follows:\n\n\nclient.set_current_atomic_multilog(\nperf_log\n);\n\n\n\n\nQuerying Pre-defined Filters\n\n\nWe can query a pre-defined filter as follows:\n\n\nauto record_stream = client.query_filter(\nlow_resources\n, 0, UINT64_MAX);\nfor (auto s = record_stream; !s.empty(); ++s) {\n  std::cout \n s.get().to_string();\n}\n\n\n\n\nThis operation returns a lazy stream of records, which automatically fetches\nmore data from the server as the clients consumes them.\n\n\nObtaining Pre-defined Aggregates\n\n\nWe can obtian a pre-defined aggregate as follows:\n\n\nstd::string value = client.get_aggregate(\nmax_latency_ms\n, 0, UINT64_MAX);\nstd::cout \n value;\n\n\n\n\nThe operation returns a string representation of the aggregate.\n\n\nObtaining Alerts from a Pre-defined Trigger\n\n\nFinally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:\n\n\nauto alert_stream = client.get_alerts(0, UINT64_MAX, \nhigh_latency_trigger\n);\nfor (auto s = alert_stream; !s.empty(); ++s) {\n  std::cout \n s.get();\n}\n\n\n\n\nSimilar to the filter query, this operation returns a lazy stream of alerts, \nwhich automatically fetches more data from the server as the clients consumes them.", 
            "title": "Online Queries"
        }, 
        {
            "location": "/online_queries/#online-queries", 
            "text": "Online queries on Confluo are executed automatically as new records are written \ninto an Atomic MultiLog. Therefore, these queries need to be pre-defined; the \nguide on  Data Storage  describes how we can define filters,\naggregates and triggers for online evaluation. In this guide, we will look\nat how we can retrieve the results of these online queries.", 
            "title": "Online Queries"
        }, 
        {
            "location": "/online_queries/#embedded-mode", 
            "text": "We begin with the embedded mode of operation. We work with the assumption that\nthe Atomic MultiLog has already been created, and filters, aggregates and triggers\nhave been added to it as outlined in  Data Storage .  To start with, we have a reference to the Atomic MultiLog as follows:  confluo::atomic_multilog* mlog = store.get_atomic_multilog( perf_log );", 
            "title": "Embedded Mode"
        }, 
        {
            "location": "/online_queries/#querying-pre-defined-filters", 
            "text": "We can query a pre-defined filter as follows:  auto record_stream = mlog- query_filter( low_resources , 0, UINT64_MAX);\nfor (auto s = record_stream; !s.empty(); s = s.tail()) {\n  std::cout   s.head().to_string();\n}  The first parameter corresponds to the name of the filter to be queried, while \nthe second and third parameters correspond to the begining timestamp and end \ntimestamp to consider for records in the filter. We've specified them to capture\nall possible values of timestamp. This operation returns a lazily evaluated record \nstream which supports functional semantics such as filter, map, etc. See Confluo's Stream API  \nfor more details on how to work with lazy streams.", 
            "title": "Querying Pre-defined Filters"
        }, 
        {
            "location": "/online_queries/#obtaining-pre-defined-aggregates", 
            "text": "We can obtian a pre-defined aggregate as follows:  auto value = mlog- get_aggregate( max_latency_ms , 0, UINT64_MAX);\nstd::cout   value.to_string();  The operation takes the name of the aggregate as its first parameter, while the \nsecond and third parameters correspond to begin and end timestmaps, as with \npre-defined filters. The query returns a  numeric  \nobject, which is a wrapper around numeric values in C++.", 
            "title": "Obtaining Pre-defined Aggregates"
        }, 
        {
            "location": "/online_queries/#obtaining-alerts-from-a-pre-defined-trigger", 
            "text": "Finally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:  auto alert_stream = mlog- get_alerts(0, UINT64_MAX,  high_latency_trigger );\nfor (auto s = alert_stream; !s.empty(); s = s.tail()) {\n  std::cout   s.head().to_string();\n}  The query takes begin and end timestamps as its first and second arguments,\nand an optional trigger name as its third argument. The query returns a lazy \nstream over generated alerts for this trigger in the specified time-range.", 
            "title": "Obtaining Alerts from a Pre-defined Trigger"
        }, 
        {
            "location": "/online_queries/#stand-alone-mode", 
            "text": "The API for Stand-alone mode of operation is quite similar to the embedded mode.\nWe only focus on the C++ Client API, since Python and Java Client APIs are\nalmost identical to the C++ Client API.  As with the embedded mode, we work with the assumption that the client is connected to the server, \nhas already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers.\nAlso, the current Atomic MultiLog for the client has been set to  perf_log  as follows:  client.set_current_atomic_multilog( perf_log );", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/online_queries/#querying-pre-defined-filters_1", 
            "text": "We can query a pre-defined filter as follows:  auto record_stream = client.query_filter( low_resources , 0, UINT64_MAX);\nfor (auto s = record_stream; !s.empty(); ++s) {\n  std::cout   s.get().to_string();\n}  This operation returns a lazy stream of records, which automatically fetches\nmore data from the server as the clients consumes them.", 
            "title": "Querying Pre-defined Filters"
        }, 
        {
            "location": "/online_queries/#obtaining-pre-defined-aggregates_1", 
            "text": "We can obtian a pre-defined aggregate as follows:  std::string value = client.get_aggregate( max_latency_ms , 0, UINT64_MAX);\nstd::cout   value;  The operation returns a string representation of the aggregate.", 
            "title": "Obtaining Pre-defined Aggregates"
        }, 
        {
            "location": "/online_queries/#obtaining-alerts-from-a-pre-defined-trigger_1", 
            "text": "Finally, we can obtain alerts generated by triggers installed on an Atomic \nMultiLog as follows:  auto alert_stream = client.get_alerts(0, UINT64_MAX,  high_latency_trigger );\nfor (auto s = alert_stream; !s.empty(); ++s) {\n  std::cout   s.get();\n}  Similar to the filter query, this operation returns a lazy stream of alerts, \nwhich automatically fetches more data from the server as the clients consumes them.", 
            "title": "Obtaining Alerts from a Pre-defined Trigger"
        }, 
        {
            "location": "/offline_queries/", 
            "text": "Offline Queries\n\n\nOffline queries in Confluo are evaluated during runtime, i.e., they are executed\n\non-the-fly\n on already written data in response to user requests. The execution\nof these queries rely on the raw data and attribute indexes. The guide on \n\nData Storage\n describes how attribute indexes can be\nadded to Atomic MultiLogs. In this guide, we will focus on how we can query\nthis data.\n\n\nEmbedded Mode\n\n\nWe start with Confluo's embedded mode of operation. We work with the assumption that\nthe Atomic MultiLog has already been created, and filters, aggregates and triggers\nhave been added to it as outlined in \nData Storage\n.\n\n\nTo start with, we have a reference to the Atomic MultiLog as follows:\n\n\nconfluo::atomic_multilog* mlog = store.get_atomic_multilog(\nperf_log\n);\n\n\n\n\nRetrieving Records\n\n\nIt is straightforward to retrieve records given their offsets:\n\n\nauto record1 = mlog-\nread(off1);\nauto record2 = mlog-\nread(off2);\nauto record3 = mlog-\nread(off3);\n\n\n\n\nEach of \nrecord1\n, \nrecord2\n, and \nrecord3\n are vectors of strings.\n\n\nEvaluating Ad-hoc Filter Expressions\n\n\nWe can query indexed attributes as follows:\n\n\nauto record_stream = mlog-\nexecute_filter(\ncpu_util\n0.5 || mem_avail\n0.5\n);\nfor (auto s = record_stream; !s.empty(); s = s.tail()) {\n  std::cout \n s.head().to_string();\n}\n\n\n\n\nThe query takes as its argument a filter expression; see the guide on \nData Storage\n \nfor details on the elements of the filter expression. The operation returns a lazily evaluated stream, \nwhich supports functional style operations like map, filter, etc. See \n\nStream API\n\nfor more details.\n\n\nStand-alone Mode\n\n\nThe API for Stand-alone mode of operation is quite similar to the embedded mode.\nWe only focus on the C++ Client API, since Python and Java Client APIs are\nalmost identical to the C++ Client API.\n\n\nAs with the embedded mode, we work with the assumption that the client is connected to the server, \nhas already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers.\nAlso, the current Atomic MultiLog for the client has been set to \nperf_log\n as follows:\n\n\nclient.set_current_atomic_multilog(\nperf_log\n);\n\n\n\n\nRetrieving Records\n\n\nIt is straightforward to retrieve records given their offsets:\n\n\nauto record1 = client.read(off1);\nauto record2 = client.read(off2);\nauto record3 = client.read(off3);\n\n\n\n\nEach of \nrecord1\n, \nrecord2\n, and \nrecord3\n are vectors of strings.\n\n\nEvaluating Ad-hoc Filter Expressions\n\n\nWe can query indexed attributes as follows:\n\n\nauto record_stream = client.execute_filter(\ncpu_util\n0.5 || mem_avail\n0.5\n);\nfor (auto s = record_stream; !s.empty(); ++s) {\n  std::cout \n s.get().to_string();\n}\n\n\n\n\nThis operation returns a lazy stream of records, which automatically fetches\nmore data from the server as the clients consumes them.", 
            "title": "Offline Queries"
        }, 
        {
            "location": "/offline_queries/#offline-queries", 
            "text": "Offline queries in Confluo are evaluated during runtime, i.e., they are executed on-the-fly  on already written data in response to user requests. The execution\nof these queries rely on the raw data and attribute indexes. The guide on  Data Storage  describes how attribute indexes can be\nadded to Atomic MultiLogs. In this guide, we will focus on how we can query\nthis data.", 
            "title": "Offline Queries"
        }, 
        {
            "location": "/offline_queries/#embedded-mode", 
            "text": "We start with Confluo's embedded mode of operation. We work with the assumption that\nthe Atomic MultiLog has already been created, and filters, aggregates and triggers\nhave been added to it as outlined in  Data Storage .  To start with, we have a reference to the Atomic MultiLog as follows:  confluo::atomic_multilog* mlog = store.get_atomic_multilog( perf_log );", 
            "title": "Embedded Mode"
        }, 
        {
            "location": "/offline_queries/#retrieving-records", 
            "text": "It is straightforward to retrieve records given their offsets:  auto record1 = mlog- read(off1);\nauto record2 = mlog- read(off2);\nauto record3 = mlog- read(off3);  Each of  record1 ,  record2 , and  record3  are vectors of strings.", 
            "title": "Retrieving Records"
        }, 
        {
            "location": "/offline_queries/#evaluating-ad-hoc-filter-expressions", 
            "text": "We can query indexed attributes as follows:  auto record_stream = mlog- execute_filter( cpu_util 0.5 || mem_avail 0.5 );\nfor (auto s = record_stream; !s.empty(); s = s.tail()) {\n  std::cout   s.head().to_string();\n}  The query takes as its argument a filter expression; see the guide on  Data Storage  \nfor details on the elements of the filter expression. The operation returns a lazily evaluated stream, \nwhich supports functional style operations like map, filter, etc. See  Stream API \nfor more details.", 
            "title": "Evaluating Ad-hoc Filter Expressions"
        }, 
        {
            "location": "/offline_queries/#stand-alone-mode", 
            "text": "The API for Stand-alone mode of operation is quite similar to the embedded mode.\nWe only focus on the C++ Client API, since Python and Java Client APIs are\nalmost identical to the C++ Client API.  As with the embedded mode, we work with the assumption that the client is connected to the server, \nhas already created the Atomic MultiLog, and added all relevant filters, aggregates and triggers.\nAlso, the current Atomic MultiLog for the client has been set to  perf_log  as follows:  client.set_current_atomic_multilog( perf_log );", 
            "title": "Stand-alone Mode"
        }, 
        {
            "location": "/offline_queries/#retrieving-records_1", 
            "text": "It is straightforward to retrieve records given their offsets:  auto record1 = client.read(off1);\nauto record2 = client.read(off2);\nauto record3 = client.read(off3);  Each of  record1 ,  record2 , and  record3  are vectors of strings.", 
            "title": "Retrieving Records"
        }, 
        {
            "location": "/offline_queries/#evaluating-ad-hoc-filter-expressions_1", 
            "text": "We can query indexed attributes as follows:  auto record_stream = client.execute_filter( cpu_util 0.5 || mem_avail 0.5 );\nfor (auto s = record_stream; !s.empty(); ++s) {\n  std::cout   s.get().to_string();\n}  This operation returns a lazy stream of records, which automatically fetches\nmore data from the server as the clients consumes them.", 
            "title": "Evaluating Ad-hoc Filter Expressions"
        }, 
        {
            "location": "/data_archival/", 
            "text": "Archiving Data\n\n\nConfluo can limit the amount of data resident in memory by archiving old data. It does this by periodically archiving records in the data log, as well as all corresponding filter and index data, up to a data log offset. Archived data is accessed in the same way as data in memory since it is memory-mapped after it is persisted. \n\n\nThis data is written to disk either in its existing format, or in a compressed representation, depending on configuration parameters. By default, the data log is compressed using LZ4. Filter and index data are delta compressed. During reads, all decompression is done under the hood. \n\n\nThe archiver does not affect any readers that are concurrently accessing data since pointers to Confluo's data structures have associated reference counts that prevent premature deallocation. Readers obtain these pointers atomically.\n\n\nUsage\n\n\nPeriodic Archival\n\n\nTo initialize a multilog with periodic archiving capabilities:\n\n\nauto archival_mode = confluo::archival::periodic_archival_mode::ON;\nstore.create_atomic_multilog(\nmy_log\n, schema, storage::IN_MEMORY, archival_mode);\n\n\n\n\nAlternatively, for an existing multilog, we can toggle periodic archival:\n\n\nmlog-\nset_periodic_archival(confluo::archival::periodic_archival_mode::ON);\nmlog-\nset_periodic_archival(confluo::archival::periodic_archival_mode::OFF);\n\n\n\n\nBy default, if archival is on, the archiver will run periodically every 5 minutes. This can be changed through configuration parameters. The maximum amount of data log data in memory can also be configured.\n\n\narchival_periodicity_ms \narchival_in_memory_datalog_window_bytes *TODO better name for above param*\n\n\n\n\nForced Archival\n\n\nRegardless of whether archival is turned on for a particular multilog, the user can force archival up to a data log offset by calling:\n\n\nmlog-\narchive(); // Archive up to the read tail\nmlog-\narchive(offset); // Archive up to any offset before the read tail\n\n\n\n\nAllocator-triggered Archival\n\n\nIn cases where the periodic archiver cannot keep up with write pressure and the maximum memory of the system is reached, the allocator will block until Confluo makes more memory available, which is done by archiving the multilogs aggressively. All multilogs are archived in their entirety to make space for newer data. This can be configured by changing:\n\n\nmax_memory", 
            "title": "Archiving Data"
        }, 
        {
            "location": "/data_archival/#archiving-data", 
            "text": "Confluo can limit the amount of data resident in memory by archiving old data. It does this by periodically archiving records in the data log, as well as all corresponding filter and index data, up to a data log offset. Archived data is accessed in the same way as data in memory since it is memory-mapped after it is persisted.   This data is written to disk either in its existing format, or in a compressed representation, depending on configuration parameters. By default, the data log is compressed using LZ4. Filter and index data are delta compressed. During reads, all decompression is done under the hood.   The archiver does not affect any readers that are concurrently accessing data since pointers to Confluo's data structures have associated reference counts that prevent premature deallocation. Readers obtain these pointers atomically.", 
            "title": "Archiving Data"
        }, 
        {
            "location": "/data_archival/#usage", 
            "text": "", 
            "title": "Usage"
        }, 
        {
            "location": "/data_archival/#periodic-archival", 
            "text": "To initialize a multilog with periodic archiving capabilities:  auto archival_mode = confluo::archival::periodic_archival_mode::ON;\nstore.create_atomic_multilog( my_log , schema, storage::IN_MEMORY, archival_mode);  Alternatively, for an existing multilog, we can toggle periodic archival:  mlog- set_periodic_archival(confluo::archival::periodic_archival_mode::ON);\nmlog- set_periodic_archival(confluo::archival::periodic_archival_mode::OFF);  By default, if archival is on, the archiver will run periodically every 5 minutes. This can be changed through configuration parameters. The maximum amount of data log data in memory can also be configured.  archival_periodicity_ms \narchival_in_memory_datalog_window_bytes *TODO better name for above param*", 
            "title": "Periodic Archival"
        }, 
        {
            "location": "/data_archival/#forced-archival", 
            "text": "Regardless of whether archival is turned on for a particular multilog, the user can force archival up to a data log offset by calling:  mlog- archive(); // Archive up to the read tail\nmlog- archive(offset); // Archive up to any offset before the read tail", 
            "title": "Forced Archival"
        }, 
        {
            "location": "/data_archival/#allocator-triggered-archival", 
            "text": "In cases where the periodic archiver cannot keep up with write pressure and the maximum memory of the system is reached, the allocator will block until Confluo makes more memory available, which is done by archiving the multilogs aggressively. All multilogs are archived in their entirety to make space for newer data. This can be configured by changing:  max_memory", 
            "title": "Allocator-triggered Archival"
        }, 
        {
            "location": "/api/", 
            "text": "Confluo API\n\n\n\n\nC++ API\n\n\nClient APIs\n\n\nC++ Client API\n\n\nPython Client API\n\n\nJava Client API", 
            "title": "Overview"
        }, 
        {
            "location": "/api/#confluo-api", 
            "text": "C++ API  Client APIs  C++ Client API  Python Client API  Java Client API", 
            "title": "Confluo API"
        }, 
        {
            "location": "/cpp_api/", 
            "text": "C++ API Documentation\n\n\nThis page will be replaced by API documentation generated by Doxygen.", 
            "title": "C++"
        }, 
        {
            "location": "/cpp_api/#c-api-documentation", 
            "text": "This page will be replaced by API documentation generated by Doxygen.", 
            "title": "C++ API Documentation"
        }, 
        {
            "location": "/client_api/", 
            "text": "Client API\n\n\n\n\nC++ Client API\n\n\nPython Client API\n\n\nJava Client API", 
            "title": "Overview"
        }, 
        {
            "location": "/client_api/#client-api", 
            "text": "C++ Client API  Python Client API  Java Client API", 
            "title": "Client API"
        }, 
        {
            "location": "/cpp_client_api/", 
            "text": "C++ Client API Documentation\n\n\nThis page will be replaced by API documentation generated by Doxygen.", 
            "title": "C++"
        }, 
        {
            "location": "/cpp_client_api/#c-client-api-documentation", 
            "text": "This page will be replaced by API documentation generated by Doxygen.", 
            "title": "C++ Client API Documentation"
        }, 
        {
            "location": "/python_client_api/", 
            "text": "Python Client API Documentation\n\n\nThis page will be replaced by API documentation generated by Sphinx.", 
            "title": "Python"
        }, 
        {
            "location": "/python_client_api/#python-client-api-documentation", 
            "text": "This page will be replaced by API documentation generated by Sphinx.", 
            "title": "Python Client API Documentation"
        }, 
        {
            "location": "/java_client_api/", 
            "text": "Java Client API Documentation\n\n\nThis page will be replaced by API documentation generated by javadocs.", 
            "title": "Java"
        }, 
        {
            "location": "/java_client_api/#java-client-api-documentation", 
            "text": "This page will be replaced by API documentation generated by javadocs.", 
            "title": "Java Client API Documentation"
        }, 
        {
            "location": "/case_studies/", 
            "text": "Case Studies\n\n\nWe have evaluated Confluo for a wide range of applications. Here, we describe the design and implementation for three such applications: \n\n\n\n\nA network monitoring and diagnosis tool\n\n\nA time-series database\n\n\nA pub-sub system\n\n\n\n\nWe also evaluate each of these applications against state-of-the art approaches in their respective domains.", 
            "title": "Overview"
        }, 
        {
            "location": "/case_studies/#case-studies", 
            "text": "We have evaluated Confluo for a wide range of applications. Here, we describe the design and implementation for three such applications:    A network monitoring and diagnosis tool  A time-series database  A pub-sub system   We also evaluate each of these applications against state-of-the art approaches in their respective domains.", 
            "title": "Case Studies"
        }, 
        {
            "location": "/network_monitoring/", 
            "text": "Network Monitoring and Diagnosis\n\n\nNetwork monitoring and diagnosis is an increasingly challenging task for network\noperators. Integrating these functionalities within the network stack at the\nend-hosts allows efficiently using end-host programmability and resources with\nminimal overheads (end-host stack process the incoming packets anyway). However,\nachieving this requires tools that support highly concurrent per-packet capture\nat line-rate, support for online queries (for monitoring purposes), and offline\nqueries (for diagnosis purposes). Confluo interface is a natural fit for\nbuilding such a tool -- flows, packets headers and header fields at an end-host\nmap perfectly to Confluo streams, records and attributes.\n\n\nFor details on the design, implementation and evaluation of the network monitoring\nand diagnosis tool, please see our \nNSDI paper\n.", 
            "title": "Network Monitoring and Diagnosis"
        }, 
        {
            "location": "/network_monitoring/#network-monitoring-and-diagnosis", 
            "text": "Network monitoring and diagnosis is an increasingly challenging task for network\noperators. Integrating these functionalities within the network stack at the\nend-hosts allows efficiently using end-host programmability and resources with\nminimal overheads (end-host stack process the incoming packets anyway). However,\nachieving this requires tools that support highly concurrent per-packet capture\nat line-rate, support for online queries (for monitoring purposes), and offline\nqueries (for diagnosis purposes). Confluo interface is a natural fit for\nbuilding such a tool -- flows, packets headers and header fields at an end-host\nmap perfectly to Confluo streams, records and attributes.  For details on the design, implementation and evaluation of the network monitoring\nand diagnosis tool, please see our  NSDI paper .", 
            "title": "Network Monitoring and Diagnosis"
        }, 
        {
            "location": "/timeseries_db/", 
            "text": "Timeseries Database\n\n\nWe describe extensions to Confluo's interface to capture time-series data and\nsupport operations similar to \nBTrDB\n on the captured data.\n\n\nImplementation\n\n\nTime-series data comprises of a stream of records, each of which is a \n(timestamp, value) pair. Confluo maintains an index on both the timestamp and\nthe value attribute on the Atomic MultiLog to support queries on time windows \nas well as more general diagnostic queries. Confluo also supports efficient \naggregate queries on the captured time-series data, but via pre-defined \naggregates and ad-hoc query execution.\n\n\nAtomic MultiLog offsets implicitly form versions for the timeseries database \u2014 \neach offset corresponds to a new version of the database and includes all \nrecords before that offset. Confluo also permits users to compute the \ndifference between two versions of the database: since database versions map \nto Atomic MultiLog offsets, Confluo fetches all records that lie between the \noffsets corresponding to the two versions.\n\n\nCompared Systems and Experimental Setup\n\n\nWe evaluate Confluo against \nBTrDB\n, \n\nCorfuDB\n and \n\nTimescaleDB\n on c4.8xlarge instances with 18 CPU \ncores and 60GB RAM. We used the \nOpen \u03bcPMU Dataset\n, \na real- world trace of voltage, current and phase readings col- lected from \nLBNL\u2019s power-grid over a 3-month period. We create a separate Atomic MultiLog\nfor each type of reading (voltage, current or phase). We run single server \nbenchmarks with 500 million records to highlight the per-server performance of\nthese systems. Requests are issued as continuous streams with 8K record batches.\n\n\nResults\n\n\n\n    \n\n\n\n\n\n\n    \nFigure:\n \nConfluoD \n&\n ConfluoR measure performance for DURABLE \n&\n DURABLE_RELAXED \n    modes respectively. ConfluoD achieves 2-20x higher throughput, 2-10x lower\n    latency for inserts, and 1.5-5x higher throughput, 5-20x lower latency for\n    time-range queries than compared systems.\n\n\n\n\n\nThe figure above shows that systems like CorfuDB and TimescaleDB achieve over 10x lower performance than BTrDB and Confluo. We emphasize that this is not a shortcoming: CorfuDB and TimescaleDB support stronger (transactional) semantics than BTrDB and Confluo. Thus, depending on desired semantics, either class of systems may be useful for an underlying application.\n\n\nConfluo, using DURABLE_RELAXED writes, is able to achieve close to 27 million inserts/second due to its cheap versioning and lock-free concurrency control. For time-range queries, almost all systems observe similar throughput since queries are served via in-memory indexes. Insertion and query latency trends are similar to throughput trends across different systems.", 
            "title": "Timeseries Database"
        }, 
        {
            "location": "/timeseries_db/#timeseries-database", 
            "text": "We describe extensions to Confluo's interface to capture time-series data and\nsupport operations similar to  BTrDB  on the captured data.", 
            "title": "Timeseries Database"
        }, 
        {
            "location": "/timeseries_db/#implementation", 
            "text": "Time-series data comprises of a stream of records, each of which is a \n(timestamp, value) pair. Confluo maintains an index on both the timestamp and\nthe value attribute on the Atomic MultiLog to support queries on time windows \nas well as more general diagnostic queries. Confluo also supports efficient \naggregate queries on the captured time-series data, but via pre-defined \naggregates and ad-hoc query execution.  Atomic MultiLog offsets implicitly form versions for the timeseries database \u2014 \neach offset corresponds to a new version of the database and includes all \nrecords before that offset. Confluo also permits users to compute the \ndifference between two versions of the database: since database versions map \nto Atomic MultiLog offsets, Confluo fetches all records that lie between the \noffsets corresponding to the two versions.", 
            "title": "Implementation"
        }, 
        {
            "location": "/timeseries_db/#compared-systems-and-experimental-setup", 
            "text": "We evaluate Confluo against  BTrDB ,  CorfuDB  and  TimescaleDB  on c4.8xlarge instances with 18 CPU \ncores and 60GB RAM. We used the  Open \u03bcPMU Dataset , \na real- world trace of voltage, current and phase readings col- lected from \nLBNL\u2019s power-grid over a 3-month period. We create a separate Atomic MultiLog\nfor each type of reading (voltage, current or phase). We run single server \nbenchmarks with 500 million records to highlight the per-server performance of\nthese systems. Requests are issued as continuous streams with 8K record batches.", 
            "title": "Compared Systems and Experimental Setup"
        }, 
        {
            "location": "/timeseries_db/#results", 
            "text": "Figure:   ConfluoD  &  ConfluoR measure performance for DURABLE  &  DURABLE_RELAXED \n    modes respectively. ConfluoD achieves 2-20x higher throughput, 2-10x lower\n    latency for inserts, and 1.5-5x higher throughput, 5-20x lower latency for\n    time-range queries than compared systems.   The figure above shows that systems like CorfuDB and TimescaleDB achieve over 10x lower performance than BTrDB and Confluo. We emphasize that this is not a shortcoming: CorfuDB and TimescaleDB support stronger (transactional) semantics than BTrDB and Confluo. Thus, depending on desired semantics, either class of systems may be useful for an underlying application.  Confluo, using DURABLE_RELAXED writes, is able to achieve close to 27 million inserts/second due to its cheap versioning and lock-free concurrency control. For time-range queries, almost all systems observe similar throughput since queries are served via in-memory indexes. Insertion and query latency trends are similar to throughput trends across different systems.", 
            "title": "Results"
        }, 
        {
            "location": "/pub_sub/", 
            "text": "Pub-sub System\n\n\nPub-sub systems like \nKafka\n and \nKinesis\n expose a publish-subscribe interface atop partitioned logs. We describe the implementation of a pub-sub system using Confluo that enables high publish and subscribe throughput for messages via lock-free concurrency.\n\n\nImplementation\n\n\nOur distributed messaging system implementation employs Kafka\u2019s interface and data model \u2014 messages are published to or subscribed from \u201ctopics\u201d, which are logically streams of messages. The system maintains a collection of topics, where messages for each topic are stored across a user-specified number of DiaLog shards.\n\n\nIn our implementation, each shard exposes a basic read and write interface. The publishers write messages in batches to shards of a particular topic, while subscribers asynchronously pull batches of messages from these shards. Similar to Kafka design, each subscriber keeps track of the objectId for its last read message in the shard, incrementing it as it consumes more messages. The key benefits of using Confluo for storing messages include:\n\n\n\n\nThe freedom from read-write contentions, and lock-free resolution of write-write contentions.\n\n\nConfluo provides an efficient means to obtain the snapshot of an entire topic, unlike Kafka. \n\n\nSupport for rich online and offline queries on message streams beyond just publish and subscribe.\n\n\n\n\nCompared Systems and Experimental Setup\n\n\nWe compare the performance for our pub-sub implementation against Apache Kafka. Since both systems are identical in terms of scaling read and write performance via multiple partitions, we ran our experiments on a single r3.8xlarge instance, using a single topic with one log partition for both systems. Reads and writes were performed for 64 byte messages, and concurrent subscribers in both systems belong to different subscriber groups, i.e., perform independent, uncoordinated reads on the partition. We mount Kafka\u2019s storage on a sufficiently sized RAM disk, ensuring that both systems operate completely in memory.\n\n\nResults\n\n\n\n    \n\n    \nFigure:\n \n Confluo observes close to linear write throughput scaling with #publishers as opposed to Kafka\u2019s sub-linear scaling, while both systems observe close to linear read throughput scaling with #subscribers. Confluo observes sub-linear increase in read and write latency with batch sizes as opposed to Kafka\u2019s linear increase (both axes are in log-scale).\n\n\n\n\n\nSince Kafka employs locks to synchronize concurrent appends, publisher write throughput suffers due to write-write contentions (Figure (left)). Confluo employs lock-free resolution for these conflicts to achieve high write throughput. Larger batches (16K messages) alleviate locking overheads in Kafka to some extent, while Confluo approaches network saturation at 16K message batches with over 4 publishers. Since reads occur without contention in both systems, read throughput scales linearly with multiple subscribers (Figure (center)). Confluo achieves higher absolute read throughput, presumably due to system overheads in Kafka and not because of a fundamental design difference. As before, read throughput for Confluo saturates at 4 subscribers and 16K message batches due to network saturation. Confluo latency increase is sub-linear with the batch size for both reads and writes, while Kafka observes super-linear increase in latency, particularly for writes (Figure (right)). Again, we suspect this effect is likely due to implementation overheads in Kafka.", 
            "title": "Pub-sub System"
        }, 
        {
            "location": "/pub_sub/#pub-sub-system", 
            "text": "Pub-sub systems like  Kafka  and  Kinesis  expose a publish-subscribe interface atop partitioned logs. We describe the implementation of a pub-sub system using Confluo that enables high publish and subscribe throughput for messages via lock-free concurrency.", 
            "title": "Pub-sub System"
        }, 
        {
            "location": "/pub_sub/#implementation", 
            "text": "Our distributed messaging system implementation employs Kafka\u2019s interface and data model \u2014 messages are published to or subscribed from \u201ctopics\u201d, which are logically streams of messages. The system maintains a collection of topics, where messages for each topic are stored across a user-specified number of DiaLog shards.  In our implementation, each shard exposes a basic read and write interface. The publishers write messages in batches to shards of a particular topic, while subscribers asynchronously pull batches of messages from these shards. Similar to Kafka design, each subscriber keeps track of the objectId for its last read message in the shard, incrementing it as it consumes more messages. The key benefits of using Confluo for storing messages include:   The freedom from read-write contentions, and lock-free resolution of write-write contentions.  Confluo provides an efficient means to obtain the snapshot of an entire topic, unlike Kafka.   Support for rich online and offline queries on message streams beyond just publish and subscribe.", 
            "title": "Implementation"
        }, 
        {
            "location": "/pub_sub/#compared-systems-and-experimental-setup", 
            "text": "We compare the performance for our pub-sub implementation against Apache Kafka. Since both systems are identical in terms of scaling read and write performance via multiple partitions, we ran our experiments on a single r3.8xlarge instance, using a single topic with one log partition for both systems. Reads and writes were performed for 64 byte messages, and concurrent subscribers in both systems belong to different subscriber groups, i.e., perform independent, uncoordinated reads on the partition. We mount Kafka\u2019s storage on a sufficiently sized RAM disk, ensuring that both systems operate completely in memory.", 
            "title": "Compared Systems and Experimental Setup"
        }, 
        {
            "location": "/pub_sub/#results", 
            "text": "Figure:    Confluo observes close to linear write throughput scaling with #publishers as opposed to Kafka\u2019s sub-linear scaling, while both systems observe close to linear read throughput scaling with #subscribers. Confluo observes sub-linear increase in read and write latency with batch sizes as opposed to Kafka\u2019s linear increase (both axes are in log-scale).   Since Kafka employs locks to synchronize concurrent appends, publisher write throughput suffers due to write-write contentions (Figure (left)). Confluo employs lock-free resolution for these conflicts to achieve high write throughput. Larger batches (16K messages) alleviate locking overheads in Kafka to some extent, while Confluo approaches network saturation at 16K message batches with over 4 publishers. Since reads occur without contention in both systems, read throughput scales linearly with multiple subscribers (Figure (center)). Confluo achieves higher absolute read throughput, presumably due to system overheads in Kafka and not because of a fundamental design difference. As before, read throughput for Confluo saturates at 4 subscribers and 16K message batches due to network saturation. Confluo latency increase is sub-linear with the batch size for both reads and writes, while Kafka observes super-linear increase in latency, particularly for writes (Figure (right)). Again, we suspect this effect is likely due to implementation overheads in Kafka.", 
            "title": "Results"
        }
    ]
}